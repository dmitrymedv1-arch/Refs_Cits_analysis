import streamlit as st
import pandas as pd
import requests
import time
from typing import List, Dict, Any, Tuple, Set
import re
from collections import Counter
import os
import concurrent.futures
from functools import lru_cache
import json
from datetime import datetime
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from openpyxl import Workbook
from openpyxl.utils.dataframe import dataframe_to_rows
from ratelimit import limits, sleep_and_retry
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
import logging
from io import BytesIO
import tempfile
import base64
from pathlib import Path

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
st.set_page_config(
    page_title="Refs/Cits Analysis - Full Professional Version",
    page_icon="üìö",
    layout="wide"
)

# –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Ñ–∞–π–ª–æ–≤
TEMP_DIR = tempfile.mkdtemp()
os.makedirs(TEMP_DIR, exist_ok=True)

def get_temp_file_path(filename: str) -> str:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–ª–Ω—ã–π –ø—É—Ç—å –∫ –≤—Ä–µ–º–µ–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É"""
    return os.path.join(TEMP_DIR, filename)

def create_download_link(file_path: str, filename: str, link_text: str) -> str:
    """–°–æ–∑–¥–∞–µ—Ç HTML —Å—Å—ã–ª–∫—É –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞"""
    try:
        with open(file_path, 'rb') as f:
            data = f.read()
        b64 = base64.b64encode(data).decode()
        href = f'<a href="data:application/octet-stream;base64,{b64}" download="{filename}" style="background-color: #4CAF50; color: white; padding: 10px 20px; text-align: center; text-decoration: none; display: inline-block; border-radius: 5px; font-weight: bold;">{link_text}</a>'
        
        # –û–¢–õ–ê–î–û–ß–ù–´–ô –í–´–í–û–î –í –ö–û–ù–°–û–õ–¨
        print(f"üîó –°–æ–∑–¥–∞–Ω–∞ HTML —Å—Å—ã–ª–∫–∞ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è:")
        print(f"   –§–∞–π–ª: {filename}")
        print(f"   –ü—É—Ç—å: {file_path}")
        print(f"   –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {len(data)} –±–∞–π—Ç")
        print(f"   HTML —Å—Å—ã–ª–∫–∞ (–ø–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤): {href[:200]}...")
        
        return href
    except Exception as e:
        error_msg = f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Å—Å—ã–ª–∫–∏: {str(e)}"
        print(f"‚ùå {error_msg}")
        return f"<p>{error_msg}</p>"

def cleanup_temp_files():
    """–û—á–∏—â–∞–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã —Å—Ç–∞—Ä—à–µ 1 —á–∞—Å–∞"""
    try:
        current_time = time.time()
        for filename in os.listdir(TEMP_DIR):
            file_path = os.path.join(TEMP_DIR, filename)
            if os.path.isfile(file_path):
                file_time = os.path.getmtime(file_path)
                if current_time - file_time > 3600:  # 1 —á–∞—Å
                    os.remove(file_path)
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {e}")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è NLTK
def initialize_nltk():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è NLTK —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
    try:
        nltk_data_dir = os.path.join(os.path.expanduser('~'), 'nltk_data')
        os.makedirs(nltk_data_dir, exist_ok=True)
        
        resources = {
            'corpora/stopwords': 'stopwords',
            'tokenizers/punkt': 'punkt'
        }
        
        for resource_path, resource_name in resources.items():
            try:
                nltk.data.find(resource_path)
            except LookupError:
                try:
                    nltk.download(resource_name, quiet=True)
                except Exception as e:
                    print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {resource_name}: {e}")
        
        return True
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ NLTK: {e}")
        return False

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º NLTK
NLTK_AVAILABLE = initialize_nltk()

class Config:
    REQUEST_TIMEOUT = 30
    MAX_RETRIES = 5
    DELAY_BETWEEN_REQUESTS = 0.1
    RETRY_DELAY = 1

class FastAffiliationProcessor:
    """–ë—ã—Å—Ç—Ä—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –∞—Ñ—Ñ–∏–ª–∏–∞—Ü–∏–π —Å –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–æ–π –ø–æ—Ö–æ–∂–∏—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π"""

    def __init__(self):
        self.common_keywords = {
            'university', 'college', 'institute', 'school', 'department', 'faculty',
            'laboratory', 'center', 'centre', 'academy', 'universit√§t', 'universitat',
            'universit√©', 'universite', 'polytechnic', 'technical', 'technology',
            'research', 'science', 'sciences', 'studies', 'medical', 'hospital',
            'clinic', 'foundation', 'corporation', 'company', 'inc', 'ltd', 'corp'
        }
        self.organization_cache = {}
        self.country_keywords = {
            'usa', 'united states', 'us', 'u.s.', 'u.s.a.', 'america',
            'uk', 'united kingdom', 'great britain', 'england', 'scotland', 'wales',
            'germany', 'deutschland', 'france', 'french', 'italy', 'italian',
            'spain', 'spanish', 'china', 'chinese', 'japan', 'japanese',
            'russia', 'russian', 'india', 'indian', 'brazil', 'brazilian',
            'canada', 'canadian', 'australia', 'australian', 'korea', 'korean'
        }

    def extract_main_organization_fast(self, affiliation: str) -> str:
        """–ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–π –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –∏–∑ –ø–æ–ª–Ω–æ–π –∞—Ñ—Ñ–∏–ª–∏–∞—Ü–∏–∏"""
        if not affiliation or affiliation in ['Unknown', 'Error', '']:
            return "Unknown"

        if affiliation in self.organization_cache:
            return self.organization_cache[affiliation]

        clean_affiliation = affiliation.strip()
        clean_affiliation = re.sub(r'\S+@\S+', '', clean_affiliation)
        clean_affiliation = re.sub(r'\d{5,}(?:-\d{4})?', '', clean_affiliation)
        clean_affiliation = re.sub(r'p\.?o\.? box \d+', '', clean_affiliation, flags=re.IGNORECASE)
        clean_affiliation = re.sub(r'\b\d+\s+[a-zA-Z]+\s+[a-zA-Z]+\b', '', clean_affiliation)

        parts = re.split(r'[,;]', clean_affiliation)
        main_org_candidates = []

        for part in parts:
            part = part.strip()
            if not part or len(part) < 5:
                continue

            part_lower = part.lower()
            has_org_keyword = any(keyword in part_lower for keyword in self.common_keywords)
            has_country = any(country in part_lower for country in self.country_keywords)

            if has_org_keyword and not has_country:
                main_org_candidates.append(part)

        if main_org_candidates:
            main_org_candidates.sort(key=len, reverse=True)
            main_org = main_org_candidates[0]
        else:
            main_org = clean_affiliation
            for part in parts:
                part = part.strip()
                if len(part) > 10 and not any(country in part.lower() for country in self.country_keywords):
                    main_org = part
                    break
            else:
                for part in parts:
                    part = part.strip()
                    if part:
                        main_org = part
                        break
                else:
                    main_org = clean_affiliation

        main_org = re.sub(r'\s+', ' ', main_org).strip()
        main_org = re.sub(r'^[^a-zA-Z0-9]*|[^a-zA-Z0-9]*$', '', main_org)

        result = main_org if main_org else "Unknown"
        self.organization_cache[affiliation] = result
        return result

    def normalize_organization_name(self, org_name: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏"""
        if not org_name or org_name == "Unknown":
            return org_name

        normalized = org_name.lower()

        remove_patterns = [
            r'^the\s+', r'\s+the$',
            r'\bdept\.?\s+of\b', r'\bdepartment\s+of\b',
            r'\bfaculty\s+of\b', r'\bschool\s+of\b',
            r'\binstitute\s+of\b', r'\binstitution\s+of\b',
            r'\bcollege\s+of\b', r'\bacademy\s+of\b',
            r'\blaboratory\b', r'\blab\b',
            r'\bcenter\b', r'\bcentre\b',
            r'\bdivision\b', r'\bgroup\b',
            r'\binc\.?$', r'\bltd\.?$', r'\bcorp\.?$', r'\bco\.?$',
            r'\bllc\.?$', r'\bgmbh\.?$'
        ]

        for pattern in remove_patterns:
            normalized = re.sub(pattern, '', normalized)

        normalized = re.sub(r'\s+', ' ', normalized).strip()
        normalized = re.sub(r'[^\w\s&]', '', normalized)

        return normalized.strip()

    def group_similar_organizations(self, organizations: List[str]) -> Dict[str, List[str]]:
        """–ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç –ø–æ—Ö–æ–∂–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏"""
        if not organizations:
            return {}

        normalized_map = {}
        for org in organizations:
            if org != "Unknown":
                normalized = self.normalize_organization_name(org)
                if normalized:
                    if normalized not in normalized_map:
                        normalized_map[normalized] = []
                    normalized_map[normalized].append(org)

        final_groups = {}
        normalized_keys = list(normalized_map.keys())

        for i, key1 in enumerate(normalized_keys):
            if key1 not in final_groups:
                final_groups[key1] = []

            final_groups[key1].extend(normalized_map[key1])

            for j, key2 in enumerate(normalized_keys[i+1:], i+1):
                if self.are_organizations_similar(key1, key2):
                    if key2 in normalized_map:
                        final_groups[key1].extend(normalized_map[key2])
                    if key2 in final_groups:
                        del final_groups[key2]

        return final_groups

    def are_organizations_similar(self, org1: str, org2: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è—é—Ç—Å—è –ª–∏ –¥–≤–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –ø–æ—Ö–æ–∂–∏–º–∏"""
        if not org1 or not org2:
            return False

        org1_lower = org1.lower()
        org2_lower = org2.lower()

        if org1_lower in org2_lower or org2_lower in org1_lower:
            return True

        words1 = set(org1_lower.split())
        words2 = set(org2_lower.split())

        if not words1 or not words2:
            return False

        stop_words = {'the', 'and', 'of', 'for', 'in', 'on', 'at', 'to', 'by'}
        words1 = words1 - stop_words
        words2 = words2 - stop_words

        if not words1 or not words2:
            return False

        intersection = words1.intersection(words2)
        union = words1.union(words2)

        similarity = len(intersection) / len(union) if union else 0

        return similarity > 0.6

    def process_affiliations_list_fast(self, affiliations: List[str]) -> Tuple[Dict[str, int], Dict[str, List[str]]]:
        """–ë—ã—Å—Ç—Ä–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–∏—Å–∫–∞ –∞—Ñ—Ñ–∏–ª–∏–∞—Ü–∏–π —Å –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–æ–π"""
        if not affiliations:
            return {}, {}

        main_organizations = []
        for aff in affiliations:
            if aff and aff not in ['Unknown', 'Error']:
                main_org = self.extract_main_organization_fast(aff)
                if main_org and main_org != "Unknown":
                    main_organizations.append(main_org)

        if not main_organizations:
            return {}, {}

        grouped_organizations = self.group_similar_organizations(main_organizations)

        group_representatives = {}
        for normalized_name, org_list in grouped_organizations.items():
            if org_list:
                representative = max(org_list, key=len)
                group_representatives[representative] = org_list

        frequency_count = {}
        for representative, org_list in group_representatives.items():
            frequency_count[representative] = len(org_list)

        return frequency_count, group_representatives

class AltmetricProcessor:
    """–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è —Å–±–æ—Ä–∞ –∞–ª—å—Ç–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö"""

    def __init__(self):
        self.altmetric_cache = {}

    def clean_doi(self, doi: str) -> str:
        """–û—á–∏—â–∞–µ—Ç DOI –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        if not doi or doi in ['Unknown', 'Error', '']:
            return None

        doi = doi.strip().lower()
        doi = re.sub(r'^(doi:)?\s*', '', doi)
        doi = re.sub(r'\s+', '', doi)
        if re.match(r'^10\.\d{4,9}/[-._;()/:a-zA-Z0-9]+$', doi):
            return doi
        return None

    @retry(stop=stop_after_attempt(Config.MAX_RETRIES),
           wait=wait_exponential(multiplier=1, min=Config.RETRY_DELAY, max=10),
           retry=retry_if_exception_type((requests.exceptions.RequestException,
                                        requests.exceptions.Timeout,
                                        requests.exceptions.ConnectionError)))
    @sleep_and_retry
    @limits(calls=15, period=1)
    def get_altmetric_data(self, doi: str) -> Dict:
        """–ü–æ–ª—É—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ –±–µ—Å–ø–ª–∞—Ç–Ω–æ–≥–æ API Altmetric –ø–æ DOI"""
        clean_doi = self.clean_doi(doi)
        if not clean_doi:
            return None

        if clean_doi in self.altmetric_cache:
            return self.altmetric_cache[clean_doi]

        url = f"https://api.altmetric.com/v1/doi/{clean_doi}"
        try:
            response = requests.get(url, timeout=Config.REQUEST_TIMEOUT)
            if response.status_code == 200:
                data = response.json()
                self.altmetric_cache[clean_doi] = data
                return data
            else:
                self.altmetric_cache[clean_doi] = None
                return None
        except requests.exceptions.RequestException:
            self.altmetric_cache[clean_doi] = None
            return None

    def get_altmetric_metrics(self, doi: str) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ –∞–ª—å—Ç–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –¥–ª—è DOI"""
        data = self.get_altmetric_data(doi)

        if not data:
            return {
                'altmetric_score': 0,
                'cited_by_posts_count': 0,
                'cited_by_tweeters_count': 0,
                'cited_by_feeds_count': 0,
                'cited_by_accounts_count': 0
            }

        return {
            'altmetric_score': data.get('score', 0),
            'cited_by_posts_count': data.get('cited_by_posts_count', 0),
            'cited_by_tweeters_count': data.get('cited_by_tweeters_count', 0),
            'cited_by_feeds_count': data.get('cited_by_feeds_count', 0),
            'cited_by_accounts_count': data.get('cited_by_accounts_count', 0)
        }

class FullCitationAnalyzer:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Academic-Analyzer/1.0 (mailto:research@university.edu)',
            'Accept': 'application/json'
        })
        
        # –ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ –∫–µ—à–∏
        self._crossref_cache = {}
        self._openalex_cache = {}
        self._article_data_cache = {}
        self._references_cache = {}
        self._citations_cache = {}
        self._unique_references_cache = {}
        self._unique_citations_cache = {}
        self.unique_ref_data_cache = {}
        self.unique_citation_data_cache = {}
        
        # –ü—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã
        self.fast_affiliation_processor = FastAffiliationProcessor()
        self.altmetric_processor = AltmetricProcessor()
        
        # NLP –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        if NLTK_AVAILABLE:
            try:
                self.stop_words = set(stopwords.words('english'))
                self.stemmer = PorterStemmer()
            except:
                self.stop_words = set()
                self.stemmer = None
        else:
            self.stop_words = {
                'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", 
                "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 
                'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 
                'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 
                'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 
                'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 
                'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 
                'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 
                'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 
                'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 
                'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 
                'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 
                'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 
                'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 
                'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', 
                "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 
                'wouldn', "wouldn't"
            }
            self.stemmer = None
        
        self.scientific_stopwords = {
            'using', 'based', 'study', 'studies', 'research', 'analysis',
            'effect', 'effects', 'properties', 'property', 'development',
            'application', 'applications', 'method', 'methods', 'approach',
            'review', 'investigation', 'characterization', 'evaluation',
            'performance', 'behavior', 'structure', 'synthesis', 'design',
            'fabrication', 'preparation', 'processing', 'measurement',
            'model', 'models', 'system', 'systems', 'technology', 'material',
            'materials', 'sample', 'samples', 'device', 'devices', 'film',
            'films', 'layer', 'layers', 'surface', 'surfaces', 'interface',
            'interfaces', 'nanoparticle', 'nanoparticles', 'nanostructure',
            'nanostructures', 'composite', 'composites', 'coating', 'coatings'
        }
        
        if self.stemmer:
            self.scientific_stopwords_stemmed = {self.stemmer.stem(word) for word in self.scientific_stopwords}
        else:
            self.scientific_stopwords_stemmed = self.scientific_stopwords

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        self.setup_logging()

    def setup_logging(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    def invalidate_cache(self, cache_type: str = None):
        """–ò–Ω–≤–∞–ª–∏–¥–∞—Ü–∏—è –∫–µ—à–∞"""
        if cache_type == 'crossref' or cache_type is None:
            self._crossref_cache.clear()
        if cache_type == 'openalex' or cache_type is None:
            self._openalex_cache.clear()
        if cache_type == 'article_data' or cache_type is None:
            self._article_data_cache.clear()
        if cache_type == 'references' or cache_type is None:
            self._references_cache.clear()
        if cache_type == 'citations' or cache_type is None:
            self._citations_cache.clear()
        if cache_type == 'unique_refs' or cache_type is None:
            self._unique_references_cache.clear()
        if cache_type == 'unique_cits' or cache_type is None:
            self._unique_citations_cache.clear()

    def validate_doi(self, doi: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å DOI"""
        if not doi or not isinstance(doi, str):
            return False
            
        doi = self.normalize_doi(doi)
        doi_pattern = r'^10\.\d{4,9}/[-._;()/:a-zA-Z0-9]+$'
        
        if not bool(re.match(doi_pattern, doi, re.IGNORECASE)):
            return False

        if len(doi) < 10:
            return False

        if re.search(r'[^\w\.\-_;()/:]', doi):
            return False

        return True
    
    def normalize_doi(self, doi: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç DOI"""
        if not doi or not isinstance(doi, str):
            return ""
            
        doi = doi.strip()
        prefixes = [
            'https://doi.org/', 'http://doi.org/', 'doi.org/',
            'doi:', 'DOI:', 'https://dx.doi.org/', 'http://dx.doi.org/',
        ]
        
        for prefix in prefixes:
            if doi.lower().startswith(prefix.lower()):
                doi = doi[len(prefix):]
                break
                
        doi = doi.split('?')[0].split('#')[0]
        return doi.strip().lower()

    def parse_doi_input(self, input_text: str) -> List[str]:
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –≤–≤–æ–¥–∞ DOI"""
        if not input_text or not isinstance(input_text, str):
            return []
            
        lines = input_text.strip().split('\n')
        dois = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            line = line.rstrip('.,;')
            doi_pattern = r'10\.\d{4,9}/[-._;()/:a-zA-Z0-9]+'
            found_dois = re.findall(doi_pattern, line, re.IGNORECASE)
            
            if found_dois:
                dois.extend(found_dois)
            else:
                if 'doi.org/' in line.lower():
                    doi_part = line.lower().split('doi.org/')[-1]
                    doi_part = doi_part.split('?')[0].split('#')[0].strip()
                    if self.validate_doi(doi_part):
                        dois.append(doi_part)
                elif line.lower().startswith('doi:'):
                    doi_part = line[4:].strip()
                    if self.validate_doi(doi_part):
                        dois.append(doi_part)
                elif self.validate_doi(line):
                    dois.append(line)
        
        cleaned_dois = []
        seen = set()
        for doi in dois:
            normalized_doi = self.normalize_doi(doi)
            if self.validate_doi(normalized_doi) and normalized_doi not in seen:
                seen.add(normalized_doi)
                cleaned_dois.append(normalized_doi)
        
        return cleaned_dois

    @retry(stop=stop_after_attempt(Config.MAX_RETRIES),
           wait=wait_exponential(multiplier=1, min=Config.RETRY_DELAY, max=10),
           retry=retry_if_exception_type((requests.exceptions.RequestException,
                                        requests.exceptions.Timeout,
                                        requests.exceptions.ConnectionError)))
    @sleep_and_retry
    @limits(calls=15, period=1)
    def get_crossref_data_batch(self, dois: List[str]) -> Dict[str, Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ Crossref –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö DOI —Å—Ä–∞–∑—É"""
        results = {}
        total_dois = len(dois)
        
        print(f"üì° –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ Crossref –¥–ª—è {total_dois} DOI...")
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            future_to_doi = {
                executor.submit(self._get_single_crossref_data, doi): doi 
                for doi in dois
            }
            
            for i, future in enumerate(concurrent.futures.as_completed(future_to_doi)):
                doi = future_to_doi[future]
                try:
                    data = future.result()
                    results[doi] = data
                except Exception as e:
                    self.logger.warning(f"Crossref error for {doi}: {e}")
                    results[doi] = {}
                
                time.sleep(Config.DELAY_BETWEEN_REQUESTS)
        
        print(f"‚úÖ –î–∞–Ω–Ω—ã–µ Crossref –ø–æ–ª—É—á–µ–Ω—ã –¥–ª—è {len(results)} DOI")
        return results

    def _get_single_crossref_data(self, doi: str) -> Dict:
        """–ü–æ–ª—É—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–¥–Ω–æ–≥–æ DOI –∏–∑ Crossref"""
        if doi in self._crossref_cache:
            return self._crossref_cache[doi]
            
        try:
            url = f"https://api.crossref.org/works/{doi}"
            response = self.session.get(url, timeout=Config.REQUEST_TIMEOUT)
            
            if response.status_code == 200:
                data = response.json().get('message', {})
                
                affiliations, countries = self.extract_affiliations_from_crossref(data)
                data['extracted_affiliations'] = affiliations
                data['extracted_countries'] = countries
                
                self._crossref_cache[doi] = data
                return data
            elif response.status_code == 404:
                self.logger.warning(f"Crossref 404 for {doi}")
                return {}
            else:
                self.logger.warning(f"Crossref {response.status_code} for {doi}")
                return {}
                
        except requests.exceptions.Timeout:
            self.logger.warning(f"Crossref timeout for {doi}")
            return {}
        except requests.exceptions.ConnectionError:
            self.logger.warning(f"Crossref connection error for {doi}")
            return {}
        except Exception as e:
            self.logger.warning(f"Crossref unexpected error for {doi}: {e}")
            return {}

    def extract_affiliations_from_crossref(self, crossref_data: Dict) -> tuple[List[str], List[str]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∞—Ñ—Ñ–∏–ª–∏–∞—Ü–∏–∏ –∏ —Å—Ç—Ä–∞–Ω—ã –∏–∑ –¥–∞–Ω–Ω—ã—Ö Crossref"""
        affiliations = set()
        countries = set()

        try:
            if 'author' in crossref_data:
                for author in crossref_data['author']:
                    if 'affiliation' in author:
                        for affil in author['affiliation']:
                            if 'name' in affil:
                                affiliation_name = affil['name'].strip()
                                if affiliation_name and affiliation_name not in ['', 'None']:
                                    main_org = self.fast_affiliation_processor.extract_main_organization_fast(affiliation_name)
                                    if main_org and main_org != "Unknown":
                                        affiliations.add(main_org)

                            country = self.extract_country_from_affiliation(affil)
                            if country:
                                countries.add(country)

        except Exception as e:
            self.logger.debug(f"Error extracting affiliations from Crossref: {e}")

        return list(affiliations), list(countries)

    def extract_country_from_affiliation(self, affiliation_data: Dict) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç—Ä–∞–Ω—É –∏–∑ –¥–∞–Ω–Ω—ã—Ö –∞—Ñ—Ñ–∏–ª–∏–∞—Ü–∏–∏ Crossref"""
        try:
            if 'country' in affiliation_data and affiliation_data['country']:
                return affiliation_data['country'].upper().strip()

            if 'name' in affiliation_data and affiliation_data['name']:
                name = affiliation_data['name'].upper()
                country_keywords = {
                    'UNITED STATES': 'US', 'USA': 'US', 'U.S.A': 'US', 'U.S.': 'US',
                    'UNITED KINGDOM': 'UK', 'UK': 'UK', 'GREAT BRITAIN': 'UK',
                    'GERMANY': 'DE', 'FRANCE': 'FR', 'CHINA': 'CN', 'JAPAN': 'JP',
                    'RUSSIA': 'RU', 'INDIA': 'IN', 'BRAZIL': 'BR', 'CANADA': 'CA',
                    'AUSTRALIA': 'AU', 'KOREA': 'KR', 'SOUTH KOREA': 'KR'
                }
                for keyword, code in country_keywords.items():
                    if keyword in name:
                        return code

        except Exception as e:
            self.logger.debug(f"Error extracting country from affiliation: {e}")

        return ""

    @retry(stop=stop_after_attempt(Config.MAX_RETRIES),
           wait=wait_exponential(multiplier=1, min=Config.RETRY_DELAY, max=10),
           retry=retry_if_exception_type((requests.exceptions.RequestException,
                                        requests.exceptions.Timeout,
                                        requests.exceptions.ConnectionError)))
    @sleep_and_retry
    @limits(calls=15, period=1)
    def get_openalex_data_batch(self, dois: List[str]) -> Dict[str, Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ OpenAlex –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö DOI —Å—Ä–∞–∑—É"""
        results = {}
        total_dois = len(dois)
        
        print(f"üì° –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ OpenAlex –¥–ª—è {total_dois} DOI...")
        
        # OpenAlex batch –∑–∞–ø—Ä–æ—Å—ã
        batch_size = 50
        for i in range(0, len(dois), batch_size):
            batch_dois = dois[i:i + batch_size]
            doi_filter = "|".join([f"https://doi.org/{doi}" for doi in batch_dois])
            url = f"https://api.openalex.org/works?filter=doi:{doi_filter}&per-page={batch_size}"
            
            try:
                response = self.session.get(url, timeout=Config.REQUEST_TIMEOUT)
                if response.status_code == 200:
                    data = response.json()
                    works = data.get('results', [])
                    
                    for work in works:
                        if work.get('doi'):
                            clean_doi = self.normalize_doi(work['doi'])
                            self._openalex_cache[clean_doi] = work
                            results[clean_doi] = work
                elif response.status_code == 429:
                    self.logger.warning("OpenAlex rate limit hit, waiting...")
                    time.sleep(5)
                    continue
                    
            except requests.exceptions.Timeout:
                self.logger.warning("OpenAlex timeout in batch request")
                continue
            except requests.exceptions.ConnectionError:
                self.logger.warning("OpenAlex connection error in batch request")
                continue
            except Exception as e:
                self.logger.warning(f"OpenAlex batch error: {e}")
                continue
                
            time.sleep(Config.DELAY_BETWEEN_REQUESTS)

        # –ò–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö DOI
        missing_dois = set(dois) - set(results.keys())
        if missing_dois:
            print(f"üì° –î–æ–∑–∞–≥—Ä—É–∑–∫–∞ {len(missing_dois)} –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ OpenAlex...")
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
                future_to_doi = {
                    executor.submit(self._get_single_openalex_data, doi): doi 
                    for doi in missing_dois
                }
                
                for future in concurrent.futures.as_completed(future_to_doi):
                    doi = future_to_doi[future]
                    try:
                        data = future.result()
                        results[doi] = data
                    except Exception:
                        results[doi] = {}
        
        print(f"‚úÖ –î–∞–Ω–Ω—ã–µ OpenAlex –ø–æ–ª—É—á–µ–Ω—ã –¥–ª—è {len(results)} DOI")
        return results

    def _get_single_openalex_data(self, doi: str) -> Dict:
        """–ü–æ–ª—É—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–¥–Ω–æ–≥–æ DOI –∏–∑ OpenAlex"""
        if doi in self._openalex_cache:
            return self._openalex_cache[doi]
            
        try:
            url = f"https://api.openalex.org/works/https://doi.org/{doi}"
            response = self.session.get(url, timeout=Config.REQUEST_TIMEOUT)
            
            if response.status_code == 200:
                data = response.json()
                self._openalex_cache[doi] = data
                return data
            elif response.status_code == 404:
                self.logger.warning(f"OpenAlex 404 for {doi}")
                return {}
            elif response.status_code == 429:
                self.logger.warning(f"OpenAlex rate limit for {doi}")
                time.sleep(2)
                return {}
            else:
                self.logger.warning(f"OpenAlex {response.status_code} for {doi}")
                return {}
                
        except requests.exceptions.Timeout:
            self.logger.warning(f"OpenAlex timeout for {doi}")
            return {}
        except requests.exceptions.ConnectionError:
            self.logger.warning(f"OpenAlex connection error for {doi}")
            return {}
        except Exception as e:
            self.logger.warning(f"OpenAlex unexpected error for {doi}: {e}")
            return {}

    def quick_doi_search(self, title: str) -> str:
        """–ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ DOI –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É (fallback –º–µ—Ç–æ–¥)"""
        if not title or title == 'Unknown':
            return None

        url = "https://api.crossref.org/works"
        params = {
            'query.title': title,
            'rows': 1,
            'select': 'DOI,title'
        }

        try:
            response = requests.get(url, params=params, timeout=Config.REQUEST_TIMEOUT)
            if response.status_code == 200:
                data = response.json()
                if data['message']['items']:
                    doi = data['message']['items'][0]['DOI']
                    if self.validate_doi(doi):
                        return doi
            return None
        except:
            return None

    def get_article_data_batch(self, dois: List[str]) -> Dict[str, Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç –ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ —Å—Ç–∞—Ç—å—è—Ö"""
        print(f"üìä –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {len(dois)} —Å—Ç–∞—Ç–µ–π...")
        
        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –æ–±–æ–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
            crossref_future = executor.submit(self.get_crossref_data_batch, dois)
            openalex_future = executor.submit(self.get_openalex_data_batch, dois)
            
            crossref_results = crossref_future.result()
            openalex_results = openalex_future.result()
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
        results = {}
        for doi in dois:
            crossref_data = crossref_results.get(doi, {})
            openalex_data = openalex_results.get(doi, {})
            altmetric_data = self.altmetric_processor.get_altmetric_metrics(doi)
            
            article_data = self._combine_article_data(doi, crossref_data, openalex_data, altmetric_data)
            results[doi] = article_data
        
        print(f"‚úÖ –î–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ç–µ–π –ø–æ–ª—É—á–µ–Ω—ã –¥–ª—è {len(results)} DOI")
        return results

    def _combine_article_data(self, doi: str, crossref_data: Dict, openalex_data: Dict, altmetric_data: Dict) -> Dict[str, Any]:
        """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫
        title = 'Unknown'
        if openalex_data and openalex_data.get('title'):
            title = openalex_data['title']
        elif crossref_data.get('title'):
            title_list = crossref_data['title']
            if title_list:
                title = title_list[0]

        # –ì–æ–¥ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
        year = 'Unknown'
        publication_year = None
        
        try:
            if openalex_data and openalex_data.get('publication_year'):
                publication_year = openalex_data['publication_year']
                year = str(publication_year)
            elif crossref_data.get('issued', {}).get('date-parts', [[]])[0]:
                year_str = str(crossref_data['issued']['date-parts'][0][0])
                if year_str.isdigit() and len(year_str) == 4:
                    publication_year = int(year_str)
                    year = year_str
                else:
                    year = year_str
                    publication_year = None
        except (ValueError, TypeError, IndexError, KeyError) as e:
            self.logger.debug(f"Error parsing year for {doi}: {e}")
            year = 'Unknown'
            publication_year = None

        # –ê–≤—Ç–æ—Ä—ã
        authors = []
        authors_with_initials = []
        if openalex_data:
            for author in openalex_data.get('authorships', []):
                name = author.get('author', {}).get('display_name', 'Unknown')
                if name != 'Unknown':
                    authors.append(name)
                    surname_with_initial = self._extract_surname_with_initial(name)
                    authors_with_initials.append(surname_with_initial)

        if not authors and crossref_data.get('author'):
            for author in crossref_data['author']:
                given = author.get('given', '')
                family = author.get('family', '')
                if given or family:
                    name = f"{given} {family}".strip()
                    authors.append(name)
                    surname_with_initial = self._extract_surname_with_initial(name)
                    authors_with_initials.append(surname_with_initial)

        authors_str = ', '.join(authors) if authors else 'Unknown'
        authors_with_initials_str = ', '.join(authors_with_initials) if authors_with_initials else 'Unknown'
        author_count = len(authors) if authors else 0

        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∂—É—Ä–Ω–∞–ª–µ
        journal_info = self._extract_journal_info(crossref_data)

        # –¶–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        citation_count_crossref = crossref_data.get('is-referenced-by-count', 0)
        citation_count_openalex = openalex_data.get('cited_by_count', 0)

        # –ê—Ñ—Ñ–∏–ª–∏–∞—Ü–∏–∏ –∏ —Å—Ç—Ä–∞–Ω—ã
        affiliations, countries = self._get_enhanced_affiliations_and_countries(openalex_data, crossref_data)

        # –†–∞—Å—á–µ—Ç –ª–µ—Ç —Å –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
        current_year = datetime.now().year
        years_since_pub = self._calculate_years_since_publication(publication_year, current_year)

        return {
            'doi': doi,
            'title': title,
            'year': year,
            'publication_year': publication_year,
            'authors': authors_str,
            'authors_with_initials': authors_with_initials_str,
            'author_count': author_count,
            'journal_full_name': journal_info['full_name'],
            'journal_abbreviation': journal_info['abbreviation'],
            'publisher': journal_info['publisher'],
            'citation_count_crossref': citation_count_crossref,
            'citation_count_openalex': citation_count_openalex,
            'annual_citation_rate_crossref': self._safe_calculate_annual_citation_rate(citation_count_crossref, publication_year),
            'annual_citation_rate_openalex': self._safe_calculate_annual_citation_rate(citation_count_openalex, publication_year),
            'years_since_publication': years_since_pub,
            'affiliations': '; '.join(affiliations),
            'countries': countries,
            'altmetric_score': altmetric_data['altmetric_score'],
            'number_of_mentions': altmetric_data['cited_by_posts_count'],
            'x_mentions': altmetric_data['cited_by_tweeters_count'],
            'rss_blogs': altmetric_data['cited_by_feeds_count'],
            'unique_accounts': altmetric_data['cited_by_accounts_count']
        }

    def _extract_journal_info(self, crossref_data: Dict) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∂—É—Ä–Ω–∞–ª–µ"""
        try:
            container_title = crossref_data.get('container-title', [])
            short_container_title = crossref_data.get('short-container-title', [])
            full_name = container_title[0] if container_title else (short_container_title[0] if short_container_title else 'Unknown')
            abbreviation = short_container_title[0] if short_container_title else (container_title[0] if container_title else 'Unknown')
            return {
                'full_name': full_name,
                'abbreviation': abbreviation,
                'publisher': crossref_data.get('publisher', 'Unknown'),
                'issn': crossref_data.get('ISSN', [None])[0]
            }
        except:
            return {
                'full_name': 'Unknown',
                'abbreviation': 'Unknown',
                'publisher': 'Unknown',
                'issn': None
            }

    def _get_enhanced_affiliations_and_countries(self, openalex_data: Dict, crossref_data: Dict) -> tuple[List[str], str]:
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∞—Ñ—Ñ–∏–ª–∏–∞—Ü–∏–π —Å –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–æ–π"""
        try:
            # –ê—Ñ—Ñ–∏–ª–∏–∞—Ü–∏–∏ –∏–∑ OpenAlex
            openalex_affiliations = set()
            openalex_countries = set()
            if openalex_data:
                for authorship in openalex_data.get('authorships', []):
                    for institution in authorship.get('institutions', []):
                        display_name = institution.get('display_name', '')
                        country_code = institution.get('country_code', '')
                        if display_name:
                            main_org = self.fast_affiliation_processor.extract_main_organization_fast(display_name)
                            if main_org and main_org != "Unknown":
                                openalex_affiliations.add(main_org)
                        if country_code:
                            openalex_countries.add(country_code.upper())

            # –ê—Ñ—Ñ–∏–ª–∏–∞—Ü–∏–∏ –∏–∑ Crossref
            crossref_affiliations = crossref_data.get('extracted_affiliations', [])
            crossref_countries = crossref_data.get('extracted_countries', [])

            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∞—Ñ—Ñ–∏–ª–∏–∞—Ü–∏–∏
            all_affiliations = list(openalex_affiliations) + crossref_affiliations
            if not all_affiliations:
                all_affiliations = ['Unknown']

            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å—Ç—Ä–∞–Ω—ã
            all_countries = openalex_countries.union(set(crossref_countries))
            final_countries = ';'.join(sorted(all_countries)) if all_countries else 'Unknown'

            return all_affiliations, final_countries

        except Exception as e:
            return ['Unknown'], 'Unknown'

    def _calculate_years_since_publication(self, publication_year: Any, current_year: int = None) -> int:
        """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π —Ä–∞—Å—á–µ—Ç –ª–µ—Ç —Å –º–æ–º–µ–Ω—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏"""
        try:
            if current_year is None:
                current_year = datetime.now().year

            if publication_year is None or publication_year == 'Unknown':
                return 1

            year_str = str(publication_year).strip()
            if not year_str or year_str == 'Unknown':
                return 1

            year_match = re.search(r'\b(19|20)\d{2}\b', year_str)
            if year_match:
                year = int(year_match.group())
            else:
                try:
                    year = int(year_str)
                except ValueError:
                    return 1

            if 1900 < year <= current_year:
                return max(1, current_year - year)
            else:
                return 1
        except (ValueError, TypeError):
            return 1

    def _safe_calculate_annual_citation_rate(self, citation_count, publication_year, current_year=None):
        """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π —Ä–∞—Å—á–µ—Ç –µ–∂–µ–≥–æ–¥–Ω–æ–π —Ü–∏—Ç–∏—Ä—É–µ–º–æ—Å—Ç–∏"""
        try:
            if not isinstance(citation_count, (int, float)) or citation_count == 0:
                return 0.0

            if publication_year is None:
                return float(citation_count)

            years = self._calculate_years_since_publication(publication_year, current_year)
            if years is None or not isinstance(years, (int, float)) or years <= 0:
                return 0.0

            return round(citation_count / years, 2)
        except (TypeError, ZeroDivisionError, ValueError):
            return 0.0

    def _extract_surname_with_initial(self, author_name: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ñ–∞–º–∏–ª–∏—é —Å –∏–Ω–∏—Ü–∏–∞–ª–∞–º–∏"""
        if not author_name or author_name in ['Unknown', 'Error']:
            return author_name
        clean_name = re.sub(r'[^\w\s\-\.]', ' ', author_name).strip()
        parts = clean_name.split()
        if not parts:
            return author_name
        surname = parts[-1]
        initial = parts[0][0].upper() if parts[0] else ''
        return f"{surname} {initial}." if initial else surname

    def get_references_batch(self, doi_list: List[str]) -> Dict[str, List[Dict]]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –¥–ª—è —Å–ø–∏—Å–∫–∞ DOI"""
        print(f"üîç –°–±–æ—Ä —Å—Å—ã–ª–æ–∫ –¥–ª—è {len(doi_list)} —Å—Ç–∞—Ç–µ–π...")
        
        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ Crossref –¥–ª—è –≤—Å–µ—Ö DOI
        crossref_data = self.get_crossref_data_batch(doi_list)
        
        all_references = {}
        for doi in doi_list:
            data = crossref_data.get(doi, {})
            references = data.get('reference', [])
            all_references[doi] = references
        
        print(f"‚úÖ –°—Å—ã–ª–∫–∏ —Å–æ–±—Ä–∞–Ω—ã –¥–ª—è {len(doi_list)} —Å—Ç–∞—Ç–µ–π")
        return all_references

    def get_citing_articles_batch(self, doi_list: List[str]) -> Dict[str, List[str]]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Ü–∏—Ç–∏—Ä—É—é—â–∏–µ —Å—Ç–∞—Ç—å–∏ –¥–ª—è —Å–ø–∏—Å–∫–∞ DOI"""
        print(f"üîç –ü–æ–∏—Å–∫ —Ü–∏—Ç–∏—Ä—É—é—â–∏—Ö —Å—Ç–∞—Ç–µ–π –¥–ª—è {len(doi_list)} —Å—Ç–∞—Ç–µ–π...")
        
        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ OpenAlex –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π
        openalex_data = self.get_openalex_data_batch(doi_list)
        
        all_citing_articles = {}
        for doi in doi_list:
            citing_dois = []
            data = openalex_data.get(doi, {})
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º OpenAlex –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π
            if data and 'cited_by_count' in data and data['cited_by_count'] > 0:
                work_id = data.get('id', '').split('/')[-1]
                if work_id:
                    try:
                        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π —Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π
                        page = 1
                        per_page = 200
                        total_retrieved = 0
                        
                        while total_retrieved < data['cited_by_count']:
                            citing_url = f"https://api.openalex.org/works?filter=cites:{work_id}&per-page={per_page}&page={page}"
                            response = self.session.get(citing_url, timeout=Config.REQUEST_TIMEOUT)
                            
                            if response.status_code == 200:
                                citing_data = response.json()
                                results = citing_data.get('results', [])
                                total_retrieved += len(results)
                                
                                for work in results:
                                    if work.get('doi'):
                                        citing_dois.append(work['doi'])
                                
                                if len(results) < per_page:
                                    break
                                    
                                page += 1
                                time.sleep(Config.DELAY_BETWEEN_REQUESTS)
                            elif response.status_code == 429:
                                self.logger.warning("OpenAlex rate limit in citations, waiting...")
                                time.sleep(5)
                                continue
                            else:
                                break
                                
                    except Exception as e:
                        self.logger.warning(f"Error getting citations for {doi}: {e}")
            
            all_citing_articles[doi] = citing_dois
        
        print(f"‚úÖ –¶–∏—Ç–∏—Ä—É—é—â–∏–µ —Å—Ç–∞—Ç—å–∏ –Ω–∞–π–¥–µ–Ω—ã –¥–ª—è {len(doi_list)} —Å—Ç–∞—Ç–µ–π")
        return all_citing_articles

    # –ê–Ω–∞–ª–∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ —á–µ—Ä–µ–∑ NLTK
    def preprocess_content_words(self, text: str) -> List[str]:
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç-—Å–ª–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        if not text or text in ['Unknown', 'Error']:
            return []
        text = text.lower()
        text = re.sub(r'[^a-zA-Z\s-]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        words = text.split()
        content_words = []
        for word in words:
            if '-' in word:
                continue
            if len(word) > 2 and word not in self.stop_words:
                if self.stemmer:
                    stemmed_word = self.stemmer.stem(word)
                    if stemmed_word not in self.scientific_stopwords_stemmed:
                        content_words.append(stemmed_word)
                else:
                    if word not in self.scientific_stopwords:
                        content_words.append(word)
        return content_words

    def extract_compound_words(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–æ—Å—Ç–∞–≤–Ω—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        if not text or text in ['Unknown', 'Error']:
            return []
        text = text.lower()
        compound_words = re.findall(r'\b[a-z]{2,}-[a-z]{2,}(?:-[a-z]{2,})*\b', text)
        return [word for word in compound_words if not any(part in self.stop_words for part in word.split('-'))]

    def extract_scientific_stopwords(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–∞—É—á–Ω—ã–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        if not text or text in ['Unknown', 'Error']:
            return []
        text = text.lower()
        text = re.sub(r'[^a-zA-Z\s]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        words = text.split()
        scientific_words = []
        for word in words:
            if len(word) > 2:
                if self.stemmer:
                    stemmed_word = self.stemmer.stem(word)
                    if stemmed_word in self.scientific_stopwords_stemmed:
                        for original_word in self.scientific_stopwords:
                            if self.stemmer.stem(original_word) == stemmed_word:
                                scientific_words.append(original_word)
                                break
                else:
                    if word in self.scientific_stopwords:
                        scientific_words.append(word)
        return scientific_words

    def analyze_titles(self, titles: List[str]) -> tuple[Counter, Counter, Counter]:
        """–ê–Ω–∞–ª–∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ —á–µ—Ä–µ–∑ NLTK"""
        content_words = []
        compound_words = []
        scientific_words = []
        valid_titles = [t for t in titles if t not in ['Unknown', 'Error']]
        for title in valid_titles:
            content_words.extend(self.preprocess_content_words(title))
            compound_words.extend(self.extract_compound_words(title))
            scientific_words.extend(self.extract_scientific_stopwords(title))
        return Counter(content_words), Counter(compound_words), Counter(scientific_words)

    # –ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ –∫–µ—à–∏ –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π
    def get_unique_references(self, references_df: pd.DataFrame) -> pd.DataFrame:
        """–ü–æ–ª—É—á–∞–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏"""
        if references_df.empty:
            return pd.DataFrame()

        cache_key = str(references_df.shape) + str(hash(str(references_df.columns.tolist())))
        if cache_key not in self._unique_references_cache:
            references_df['ref_id'] = references_df['doi'].fillna('') + '|' + references_df['title'].fillna('')
            unique_df = references_df.drop_duplicates(subset=['ref_id'], keep='first').drop(columns=['ref_id'])
            self._unique_references_cache[cache_key] = unique_df
        return self._unique_references_cache[cache_key]

    def get_unique_citations(self, citations_df: pd.DataFrame) -> pd.DataFrame:
        """–ü–æ–ª—É—á–∞–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        if citations_df.empty:
            return pd.DataFrame()

        cache_key = str(citations_df.shape) + str(hash(str(citations_df.columns.tolist())))
        if cache_key not in self._unique_citations_cache:
            citations_df['citation_id'] = citations_df['doi'].fillna('') + '|' + citations_df['title'].fillna('')
            unique_df = citations_df.drop_duplicates(subset=['citation_id'], keep='first').drop(columns=['citation_id'])
            self._unique_citations_cache[cache_key] = unique_df
        return self._unique_citations_cache[cache_key]

    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    def analyze_authors_frequency(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ê–Ω–∞–ª–∏–∑ —á–∞—Å—Ç–æ—Ç—ã –∞–≤—Ç–æ—Ä–æ–≤"""
        if df.empty:
            return pd.DataFrame()
            
        try:
            total_refs = len(df)
            unique_df = self.get_unique_references(df) if 'source_doi' in df.columns else df
            total_unique = len(unique_df)
            
            authors_total = df['authors_with_initials'].str.split(',', expand=True).stack()
            authors_total = authors_total[authors_total.str.strip().isin(['Unknown', 'Error']) == False]
            author_freq_total = authors_total.value_counts().reset_index()
            author_freq_total.columns = ['author_with_initial', 'frequency_total']
            author_freq_total['percentage_total'] = round(author_freq_total['frequency_total'] / total_refs * 100, 2)

            authors_unique = unique_df['authors_with_initials'].str.split(',', expand=True).stack()
            authors_unique = authors_unique[authors_unique.str.strip().isin(['Unknown', 'Error']) == False]
            author_freq_unique = authors_unique.value_counts().reset_index()
            author_freq_unique.columns = ['author_with_initial', 'frequency_unique']
            author_freq_unique['percentage_unique'] = round(author_freq_unique['frequency_unique'] / total_unique * 100, 2)

            author_freq = author_freq_total.merge(author_freq_unique, on='author_with_initial', how='outer').fillna(0)
            return author_freq[['author_with_initial', 'frequency_total', 'percentage_total', 'frequency_unique', 'percentage_unique']].sort_values('frequency_total', ascending=False)
        except:
            return pd.DataFrame()

    def analyze_journals_frequency(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ê–Ω–∞–ª–∏–∑ —á–∞—Å—Ç–æ—Ç—ã –∂—É—Ä–Ω–∞–ª–æ–≤"""
        if df.empty:
            return pd.DataFrame()
            
        try:
            total_refs = len(df)
            unique_df = self.get_unique_references(df) if 'source_doi' in df.columns else df
            total_unique = len(unique_df)
            
            journals_total = df['journal_abbreviation']
            journals_total = journals_total[journals_total.isin(['Unknown', 'Error']) == False]
            journal_freq_total = journals_total.value_counts().reset_index()
            journal_freq_total.columns = ['journal_abbreviation', 'frequency_total']
            journal_freq_total['percentage_total'] = round(journal_freq_total['frequency_total'] / total_refs * 100, 2)

            journals_unique = unique_df['journal_abbreviation']
            journals_unique = journals_unique[journals_unique.isin(['Unknown', 'Error']) == False]
            journal_freq_unique = journals_unique.value_counts().reset_index()
            journal_freq_unique.columns = ['journal_abbreviation', 'frequency_unique']
            journal_freq_unique['percentage_unique'] = round(journal_freq_unique['frequency_unique'] / total_unique * 100, 2)

            journal_freq = journal_freq_total.merge(journal_freq_unique, on='journal_abbreviation', how='outer').fillna(0)

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π
            journal_citation_metrics = []
            for journal in journal_freq['journal_abbreviation']:
                journal_articles = unique_df[unique_df['journal_abbreviation'] == journal]

                total_crossref_citations = journal_articles['citation_count_crossref'].sum()
                total_openalex_citations = journal_articles['citation_count_openalex'].sum()

                avg_crossref_citations = journal_articles['citation_count_crossref'].mean() if len(journal_articles) > 0 else 0
                avg_openalex_citations = journal_articles['citation_count_openalex'].mean() if len(journal_articles) > 0 else 0

                journal_citation_metrics.append({
                    'journal_abbreviation': journal,
                    'total_crossref_citations': total_crossref_citations,
                    'total_openalex_citations': total_openalex_citations,
                    'avg_crossref_citations': round(avg_crossref_citations, 2),
                    'avg_openalex_citations': round(avg_openalex_citations, 2)
                })

            journal_metrics_df = pd.DataFrame(journal_citation_metrics)
            journal_freq = journal_freq.merge(journal_metrics_df, on='journal_abbreviation', how='left').fillna(0)

            return journal_freq[['journal_abbreviation', 'frequency_total', 'percentage_total',
                               'frequency_unique', 'percentage_unique', 'total_crossref_citations',
                               'total_openalex_citations', 'avg_crossref_citations', 'avg_openalex_citations']].sort_values('frequency_total', ascending=False)
        except:
            return pd.DataFrame()

    def analyze_affiliations_frequency(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ê–Ω–∞–ª–∏–∑ —á–∞—Å—Ç–æ—Ç—ã –∞—Ñ—Ñ–∏–ª–∏–∞—Ü–∏–π"""
        if df.empty:
            return pd.DataFrame()

        try:
            total_refs = len(df)
            unique_df = self.get_unique_references(df) if 'source_doi' in df.columns else df
            total_unique = len(unique_df)

            all_affiliations = []
            for affil_string in df['affiliations']:
                if pd.isna(affil_string) or affil_string in ['Unknown', 'Error', '']:
                    continue
                try:
                    affil_list = affil_string.split(';')
                    for affil in affil_list:
                        clean_affil = affil.strip()
                        if clean_affil and clean_affil not in ['Unknown', 'Error']:
                            all_affiliations.append(clean_affil)
                except:
                    pass

            if not all_affiliations:
                return pd.DataFrame(columns=['affiliation', 'frequency_total', 'percentage_total', 'frequency_unique', 'percentage_unique'])

            affiliation_frequencies, grouped_organizations = self.fast_affiliation_processor.process_affiliations_list_fast(all_affiliations)

            affil_data = []
            for affil, freq in affiliation_frequencies.items():
                percentage_total = round(freq / total_refs * 100, 2) if total_refs > 0 else 0
                affil_data.append({
                    'affiliation': affil,
                    'frequency_total': freq,
                    'percentage_total': percentage_total
                })

            unique_affiliations = []
            for affil_string in unique_df['affiliations']:
                if affil_string and affil_string not in ['Unknown', 'Error']:
                    affil_list = affil_string.split(';')
                    unique_affiliations.extend([affil.strip() for affil in affil_list if affil.strip()])

            if unique_affiliations:
                unique_frequencies, _ = self.fast_affiliation_processor.process_affiliations_list_fast(unique_affiliations)
                affil_df = pd.DataFrame(affil_data)
                for affil, freq in unique_frequencies.items():
                    percentage_unique = round(freq / total_unique * 100, 2) if total_unique > 0 else 0
                    if affil in affil_df['affiliation'].values:
                        affil_df.loc[affil_df['affiliation'] == affil, 'frequency_unique'] = freq
                        affil_df.loc[affil_df['affiliation'] == affil, 'percentage_unique'] = percentage_unique
                    else:
                        affil_df = pd.concat([affil_df, pd.DataFrame([{
                            'affiliation': affil,
                            'frequency_total': 0,
                            'percentage_total': 0,
                            'frequency_unique': freq,
                            'percentage_unique': percentage_unique
                        }])], ignore_index=True)
            else:
                affil_df = pd.DataFrame(affil_data)
                affil_df['frequency_unique'] = affil_df['frequency_total']
                affil_df['percentage_unique'] = affil_df['percentage_total']

            affil_df = affil_df.fillna(0)
            return affil_df[['affiliation', 'frequency_total', 'percentage_total', 'frequency_unique', 'percentage_unique']].sort_values('frequency_total', ascending=False)

        except:
            return pd.DataFrame()

    def analyze_countries_frequency(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ê–Ω–∞–ª–∏–∑ —á–∞—Å—Ç–æ—Ç—ã —Å—Ç—Ä–∞–Ω"""
        if df.empty:
            return pd.DataFrame()

        try:
            total_refs = len(df)
            unique_df = self.get_unique_references(df) if 'source_doi' in df.columns else df
            total_unique = len(unique_df)

            # –ê–Ω–∞–ª–∏–∑ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Å—Ç—Ä–∞–Ω
            single_countries = []
            collaborations = []

            for countries in df['countries']:
                if countries not in ['Unknown', 'Error']:
                    country_list = [c.strip() for c in countries.split(';')]
                    if len(country_list) == 1:
                        single_countries.extend(country_list)
                    else:
                        collaborations.append(countries)

            # –ß–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Å—Ç—Ä–∞–Ω
            single_country_counter = Counter(single_countries)
            single_country_freq = pd.DataFrame({
                'countries': list(single_country_counter.keys()),
                'type': ['single'] * len(single_country_counter),
                'frequency_total': list(single_country_counter.values())
            })
            single_country_freq['percentage_total'] = round(single_country_freq['frequency_total'] / total_refs * 100, 2)

            # –ß–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å –∫–æ–ª–ª–∞–±–æ—Ä–∞—Ü–∏–π
            collaboration_counter = Counter(collaborations)
            collaboration_freq = pd.DataFrame({
                'countries': list(collaboration_counter.keys()),
                'type': ['collaboration'] * len(collaboration_counter),
                'frequency_total': list(collaboration_counter.values())
            })
            collaboration_freq['percentage_total'] = round(collaboration_freq['frequency_total'] / total_refs * 100, 2)

            # –û–±—ä–µ–¥–∏–Ω—è–µ–º
            country_freq_total = pd.concat([single_country_freq, collaboration_freq], ignore_index=True)

            # –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π
            single_countries_unique = []
            collaborations_unique = []

            for countries in unique_df['countries']:
                if countries not in ['Unknown', 'Error']:
                    country_list = [c.strip() for c in countries.split(';')]
                    if len(country_list) == 1:
                        single_countries_unique.extend(country_list)
                    else:
                        collaborations_unique.append(countries)

            single_country_counter_unique = Counter(single_countries_unique)
            collaboration_counter_unique = Counter(collaborations_unique)

            single_country_freq_unique = pd.DataFrame({
                'countries': list(single_country_counter_unique.keys()),
                'frequency_unique': list(single_country_counter_unique.values())
            })
            single_country_freq_unique['percentage_unique'] = round(single_country_freq_unique['frequency_unique'] / total_unique * 100, 2)

            collaboration_freq_unique = pd.DataFrame({
                'countries': list(collaboration_counter_unique.keys()),
                'frequency_unique': list(collaboration_counter_unique.values())
            })
            collaboration_freq_unique['percentage_unique'] = round(collaboration_freq_unique['frequency_unique'] / total_unique * 100, 2)

            country_freq_unique = pd.concat([
                single_country_freq_unique.assign(type='single'),
                collaboration_freq_unique.assign(type='collaboration')
            ], ignore_index=True)

            country_freq = country_freq_total.merge(country_freq_unique, on=['countries', 'type'], how='outer').fillna(0)

            return country_freq[['countries', 'type', 'frequency_total', 'percentage_total', 'frequency_unique', 'percentage_unique']].sort_values('frequency_total', ascending=False)
        except:
            return pd.DataFrame()

    def analyze_year_distribution(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ –≥–æ–¥–∞–º"""
        if df.empty:
            return pd.DataFrame()

        try:
            total_refs = len(df)
            unique_df = self.get_unique_references(df) if 'source_doi' in df.columns else df
            total_unique = len(unique_df)
            
            years_total = pd.to_numeric(df['year'], errors='coerce')
            years_total = years_total[years_total.notna() & years_total.between(1900, 2026)].astype(int)
            year_counts_total = years_total.value_counts().reset_index()
            year_counts_total.columns = ['year', 'frequency_total']
            year_counts_total['percentage_total'] = round(year_counts_total['frequency_total'] / total_refs * 100, 2)

            years_unique = pd.to_numeric(unique_df['year'], errors='coerce')
            years_unique = years_unique[years_unique.notna() & years_unique.between(1900, 2026)].astype(int)
            year_counts_unique = years_unique.value_counts().reset_index()
            year_counts_unique.columns = ['year', 'frequency_unique']
            year_counts_unique['percentage_unique'] = round(year_counts_unique['frequency_unique'] / total_unique * 100, 2)

            year_counts = year_counts_total.merge(year_counts_unique, on='year', how='outer').fillna(0)
            return year_counts[['year', 'frequency_total', 'percentage_total', 'frequency_unique', 'percentage_unique']].sort_values('year', ascending=False)
        except:
            return pd.DataFrame()

    def analyze_five_year_periods(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ê–Ω–∞–ª–∏–∑ –ø—è—Ç–∏–ª–µ—Ç–Ω–∏—Ö –ø–µ—Ä–∏–æ–¥–æ–≤"""
        if df.empty:
            return pd.DataFrame()

        try:
            total_refs = len(df)
            unique_df = self.get_unique_references(df) if 'source_doi' in df.columns else df
            total_unique = len(unique_df)

            start_year = 1900
            current_year = datetime.now().year + 4
            period_starts = list(range(start_year, current_year + 1, 5))
            bins = period_starts + [period_starts[-1] + 5]
            labels = [f"{s}-{s+4}" for s in period_starts]

            years_total = pd.to_numeric(df['year'], errors='coerce')
            years_total = years_total[years_total.notna() & years_total.between(1900, current_year)].astype(int)
            period_counts_total = pd.cut(years_total, bins=bins, labels=labels, right=False).astype(str)
            period_df_total = period_counts_total.value_counts().reset_index()
            period_df_total.columns = ['period', 'frequency_total']
            period_df_total['percentage_total'] = round(period_df_total['frequency_total'] / total_refs * 100, 2)
            period_df_total['period'] = period_df_total['period'].astype(str)

            years_unique = pd.to_numeric(unique_df['year'], errors='coerce')
            years_unique = years_unique[years_unique.notna() & years_unique.between(1900, current_year)].astype(int)
            period_counts_unique = pd.cut(years_unique, bins=bins, labels=labels, right=False).astype(str)
            period_df_unique = period_counts_unique.value_counts().reset_index()
            period_df_unique.columns = ['period', 'frequency_unique']
            period_df_unique['percentage_unique'] = round(period_df_unique['frequency_unique'] / total_unique * 100, 2)
            period_df_unique['period'] = period_df_unique['period'].astype(str)

            period_df = period_df_total.merge(period_df_unique, on='period', how='outer').fillna(0)
            return period_df[['period', 'frequency_total', 'percentage_total', 'frequency_unique', 'percentage_unique']].sort_values('period', ascending=False)
        except:
            return pd.DataFrame()

    def find_duplicate_references(self, references_df: pd.DataFrame) -> pd.DataFrame:
        """–ù–∞—Ö–æ–¥–∏—Ç –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è —Å—Å—ã–ª–∫–∏"""
        try:
            if references_df.empty:
                return pd.DataFrame()

            references_df['ref_id'] = references_df['doi'].fillna('') + '|' + references_df['title'].fillna('')
            ref_counts = references_df.groupby('ref_id')['source_doi'].nunique().reset_index()
            duplicate_ref_ids = ref_counts[ref_counts['source_doi'] > 1]['ref_id']
            if duplicate_ref_ids.empty:
                columns = list(references_df.columns) + ['frequency']
                columns.remove('ref_id')
                return pd.DataFrame(columns=columns)
            frequency_map = references_df['ref_id'].value_counts().to_dict()
            duplicates = references_df[references_df['ref_id'].isin(duplicate_ref_ids)].copy()
            duplicates = duplicates.drop_duplicates(subset=['ref_id'], keep='first')
            duplicates = duplicates[~((duplicates['doi'].isna()) & (duplicates['title'] == 'Unknown'))]
            duplicates['frequency'] = duplicates['ref_id'].map(frequency_map)
            duplicates = duplicates.drop(columns=['ref_id'])
            return duplicates.sort_values(['frequency', 'doi'], ascending=[False, True])
        except:
            return pd.DataFrame()

    def find_duplicate_citations(self, citations_df: pd.DataFrame) -> pd.DataFrame:
        """–ù–∞—Ö–æ–¥–∏—Ç –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        try:
            if citations_df.empty:
                return pd.DataFrame()

            citations_df['citation_id'] = citations_df['doi'].fillna('') + '|' + citations_df['title'].fillna('')

            citation_counts = citations_df.groupby('citation_id')['source_doi'].nunique().reset_index()
            duplicate_citation_ids = citation_counts[citation_counts['source_doi'] > 1]['citation_id']

            if duplicate_citation_ids.empty:
                columns = list(citations_df.columns) + ['frequency']
                columns.remove('citation_id')
                return pd.DataFrame(columns=columns)

            frequency_map = citation_counts.set_index('citation_id')['source_doi'].to_dict()

            duplicates = citations_df[citations_df['citation_id'].isin(duplicate_citation_ids)].copy()
            duplicates = duplicates.drop_duplicates(subset=['citation_id'], keep='first')

            duplicates = duplicates[~((duplicates['doi'].isna()) & (duplicates['title'] == 'Unknown'))]

            duplicates['frequency'] = duplicates['citation_id'].map(frequency_map)
            duplicates = duplicates.drop(columns=['citation_id'])

            return duplicates.sort_values(['frequency', 'doi'], ascending=[False, True])
        except:
            return pd.DataFrame()

    # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∞–Ω–∞–ª–∏–∑–∞ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –ø—Ä–æ–≥—Ä–µ—Å—Å–æ–º
    def analyze_references_comprehensive(self, doi_list: List[str]) -> pd.DataFrame:
        """–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å—Å—ã–ª–æ–∫ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –ø—Ä–æ–≥—Ä–µ—Å—Å–æ–º"""
        st.info(f"üîç –ù–∞—á–∏–Ω–∞–µ–º –∞–Ω–∞–ª–∏–∑ —Å—Å—ã–ª–æ–∫ –¥–ª—è {len(doi_list)} —Å—Ç–∞—Ç–µ–π...")
        
        # –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        main_progress = st.progress(0)
        main_status = st.empty()
        step_progress = st.empty()
        step_status = st.empty()
        
        # –®–∞–≥ 1: –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏—Å—Ö–æ–¥–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π
        main_status.text("üìä –®–∞–≥ 1/5: –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏—Å—Ö–æ–¥–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π...")
        step_status.text("–ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π...")
        source_articles_data = self.get_article_data_batch(doi_list)
        main_progress.progress(0.2)
        
        # –®–∞–≥ 2: –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏
        main_status.text("üìä –®–∞–≥ 2/5: –°–±–æ—Ä —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç–∞—Ç—å–∏...")
        step_status.text("–°–±–æ—Ä –±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏–∏...")
        all_references = self.get_references_batch(doi_list)
        main_progress.progress(0.4)
        
        # –®–∞–≥ 3: –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ DOI —Å—Å—ã–ª–æ–∫
        main_status.text("üìä –®–∞–≥ 3/5: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫...")
        step_status.text("–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ DOI –∏–∑ —Å—Å—ã–ª–æ–∫...")
        all_reference_dois = set()
        reference_titles = []
        
        total_refs = sum(len(refs) for refs in all_references.values())
        processed_refs = 0
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –∏—Å–ø–æ–ª—å–∑—É–µ–º .items() –≤–º–µ—Å—Ç–æ .values()
        for source_doi, references in all_references.items():
            for ref in references:
                ref_doi = ref.get('DOI')
                title = ref.get('article-title', 'Unknown')
                reference_titles.append(title)
                
                if ref_doi and self.validate_doi(ref_doi):
                    all_reference_dois.add(ref_doi)
                processed_refs += 1
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 10 —Å—Å—ã–ª–æ–∫
                if processed_refs % 10 == 0:
                    progress_pct = processed_refs / total_refs * 100
                    step_progress.progress(progress_pct / 100)
                    step_status.text(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed_refs}/{total_refs} —Å—Å—ã–ª–æ–∫ ({progress_pct:.1f}%)")
        
        main_progress.progress(0.6)
        
        # –®–∞–≥ 4: –ü–æ–∏—Å–∫ DOI –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º –¥–ª—è —Å—Å—ã–ª–æ–∫ –±–µ–∑ DOI
        main_status.text("üìä –®–∞–≥ 4/5: –ü–æ–∏—Å–∫ DOI –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º...")
        step_status.text("–ü–æ–∏—Å–∫ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö DOI...")
        
        titles_to_search = [title for title in reference_titles if title != 'Unknown']
        total_titles = len(titles_to_search)
        found_dois = 0
        
        for i, title in enumerate(titles_to_search):
            found_doi = self.quick_doi_search(title)
            if found_doi:
                all_reference_dois.add(found_doi)
                found_dois += 1
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
            if i % 5 == 0:
                progress_pct = (i + 1) / total_titles * 100
                step_progress.progress(progress_pct / 100)
                step_status.text(f"–ù–∞–π–¥–µ–Ω–æ DOI: {found_dois}/{i+1} ({progress_pct:.1f}%)")
            
            time.sleep(0.1)  # –ß—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—Ç—å API
        
        main_progress.progress(0.8)
        
        # –®–∞–≥ 5: –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –≤—Å–µ—Ö —Å—Å—ã–ª–æ–∫
        main_status.text("üìä –®–∞–≥ 5/5: –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫...")
        step_status.text("–ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫...")
        
        if all_reference_dois:
            reference_dois_list = list(all_reference_dois)
            total_ref_dois = len(reference_dois_list)
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–∞—Ç—á–∏ –¥–ª—è –ª—É—á—à–µ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            batch_size = 50
            reference_articles_data = {}
            
            for i in range(0, total_ref_dois, batch_size):
                batch_dois = reference_dois_list[i:i + batch_size]
                batch_data = self.get_article_data_batch(batch_dois)
                reference_articles_data.update(batch_data)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
                progress_pct = min((i + batch_size) / total_ref_dois * 100, 100)
                step_progress.progress(progress_pct / 100)
                step_status.text(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {min(i + batch_size, total_ref_dois)}/{total_ref_dois} —Å—Å—ã–ª–æ–∫ ({progress_pct:.1f}%)")
        else:
            reference_articles_data = {}
        
        main_progress.progress(1.0)
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
        main_status.text("üìä –°–±–æ—Ä–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
        step_status.text("–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...")
        
        all_data = []
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ —Å—Ç–∞—Ç—å–∏
        for doi, data in source_articles_data.items():
            data['type'] = 'source'
            data['source_doi'] = doi
            all_data.append(data)
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å—Å—ã–ª–∫–∏
        processed_connections = 0
        total_connections = sum(len(refs) for refs in all_references.values())
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: —Å–Ω–æ–≤–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º .items() –¥–ª—è –∏—Ç–µ—Ä–∞—Ü–∏–∏
        for source_doi, references in all_references.items():
            for i, ref in enumerate(references):
                ref_doi = ref.get('DOI')
                title = ref.get('article-title', 'Unknown')
                
                if ref_doi and self.validate_doi(ref_doi):
                    ref_data = reference_articles_data.get(ref_doi, {})
                    if ref_data:
                        ref_data['type'] = 'reference'
                        ref_data['source_doi'] = source_doi
                        ref_data['position'] = i + 1
                        all_data.append(ref_data)
                else:
                    # Fallback –æ–±—Ä–∞–±–æ—Ç–∫–∞
                    found_doi = self.quick_doi_search(title)
                    if found_doi and found_doi in reference_articles_data:
                        ref_data = reference_articles_data[found_doi].copy()
                        ref_data['type'] = 'reference'
                        ref_data['source_doi'] = source_doi
                        ref_data['position'] = i + 1
                        all_data.append(ref_data)
                    else:
                        # –ë–∞–∑–æ–≤–∞—è –∑–∞–ø–∏—Å—å –¥–ª—è —Å—Å—ã–ª–∫–∏ –±–µ–∑ DOI
                        all_data.append({
                            'source_doi': source_doi,
                            'position': i + 1,
                            'doi': ref_doi,
                            'title': title,
                            'authors': 'Unknown',
                            'authors_with_initials': 'Unknown',
                            'author_count': 0,
                            'year': ref.get('year', 'Unknown'),
                            'journal_full_name': 'Unknown',
                            'journal_abbreviation': 'Unknown',
                            'publisher': 'Unknown',
                            'citation_count_crossref': 0,
                            'citation_count_openalex': 0,
                            'annual_citation_rate_crossref': 0,
                            'annual_citation_rate_openalex': 0,
                            'years_since_publication': 1,
                            'affiliations': 'Unknown',
                            'countries': 'Unknown',
                            'altmetric_score': 0,
                            'number_of_mentions': 0,
                            'x_mentions': 0,
                            'rss_blogs': 0,
                            'unique_accounts': 0,
                            'type': 'reference',
                            'error': f"Invalid DOI: {ref_doi}" if ref_doi else "No DOI found"
                        })
                
                processed_connections += 1
                if processed_connections % 10 == 0:
                    progress_pct = processed_connections / total_connections * 100
                    step_progress.progress(progress_pct / 100)
                    step_status.text(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å–≤—è–∑–µ–π: {processed_connections}/{total_connections} ({progress_pct:.1f}%)")
        
        main_status.text("‚úÖ –ê–Ω–∞–ª–∏–∑ —Å—Å—ã–ª–æ–∫ –∑–∞–≤–µ—Ä—à–µ–Ω!")
        step_status.text("")
        step_progress.empty()
        
        # –°–æ–∑–¥–∞–µ–º DataFrame –∏ –¥–æ–±–∞–≤–ª—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏
        result_df = pd.DataFrame(all_data)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏, –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
        expected_columns = [
            'doi', 'title', 'year', 'publication_year', 'authors', 'authors_with_initials', 
            'author_count', 'journal_full_name', 'journal_abbreviation', 'publisher',
            'citation_count_crossref', 'citation_count_openalex', 'annual_citation_rate_crossref',
            'annual_citation_rate_openalex', 'years_since_publication', 'affiliations', 
            'countries', 'altmetric_score', 'number_of_mentions', 'x_mentions', 'rss_blogs',
            'unique_accounts', 'type', 'source_doi', 'position', 'error'
        ]
        
        for col in expected_columns:
            if col not in result_df.columns:
                result_df[col] = None
        
        return result_df

    def analyze_citations_comprehensive(self, doi_list: List[str]) -> pd.DataFrame:
        """–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –ø—Ä–æ–≥—Ä–µ—Å—Å–æ–º"""
        st.info(f"üîç –ù–∞—á–∏–Ω–∞–µ–º –∞–Ω–∞–ª–∏–∑ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π –¥–ª—è {len(doi_list)} —Å—Ç–∞—Ç–µ–π...")
        
        # –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        main_progress = st.progress(0)
        main_status = st.empty()
        step_progress = st.empty()
        step_status = st.empty()
        
        # –®–∞–≥ 1: –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏—Å—Ö–æ–¥–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π
        main_status.text("üìä –®–∞–≥ 1/4: –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏—Å—Ö–æ–¥–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π...")
        step_status.text("–ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π...")
        source_articles_data = self.get_article_data_batch(doi_list)
        main_progress.progress(0.25)
        
        # –®–∞–≥ 2: –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ü–∏—Ç–∏—Ä—É—é—â–∏–µ —Å—Ç–∞—Ç—å–∏
        main_status.text("üìä –®–∞–≥ 2/4: –ü–æ–∏—Å–∫ —Ü–∏—Ç–∏—Ä—É—é—â–∏—Ö —Å—Ç–∞—Ç–µ–π...")
        step_status.text("–ü–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π, —Ü–∏—Ç–∏—Ä—É—é—â–∏—Ö –∏—Å—Ö–æ–¥–Ω—ã–µ...")
        all_citing_articles = self.get_citing_articles_batch(doi_list)
        main_progress.progress(0.5)
        
        # –®–∞–≥ 3: –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ DOI —Ü–∏—Ç–∏—Ä—É—é—â–∏—Ö —Å—Ç–∞—Ç–µ–π
        main_status.text("üìä –®–∞–≥ 3/4: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π...")
        step_status.text("–°–±–æ—Ä DOI —Ü–∏—Ç–∏—Ä—É—é—â–∏—Ö —Å—Ç–∞—Ç–µ–π...")
        all_citing_dois = set()
        for citing_dois in all_citing_articles.values():
            all_citing_dois.update(citing_dois)
        
        main_progress.progress(0.75)
        
        # –®–∞–≥ 4: –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –≤—Å–µ—Ö —Ü–∏—Ç–∏—Ä—É—é—â–∏—Ö —Å—Ç–∞—Ç–µ–π
        main_status.text("üìä –®–∞–≥ 4/4: –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Ü–∏—Ç–∏—Ä—É—é—â–∏—Ö —Å—Ç–∞—Ç–µ–π...")
        step_status.text("–ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Ü–∏—Ç–∏—Ä—É—é—â–∏—Ö —Å—Ç–∞—Ç–µ–π...")
        
        if all_citing_dois:
            citing_dois_list = list(all_citing_dois)
            total_citing_dois = len(citing_dois_list)
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–∞—Ç—á–∏ –¥–ª—è –ª—É—á—à–µ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            batch_size = 50
            citing_articles_data = {}
            
            for i in range(0, total_citing_dois, batch_size):
                batch_dois = citing_dois_list[i:i + batch_size]
                batch_data = self.get_article_data_batch(batch_dois)
                citing_articles_data.update(batch_data)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
                progress_pct = min((i + batch_size) / total_citing_dois * 100, 100)
                step_progress.progress(progress_pct / 100)
                step_status.text(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {min(i + batch_size, total_citing_dois)}/{total_citing_dois} —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π ({progress_pct:.1f}%)")
        else:
            citing_articles_data = {}
        
        main_progress.progress(1.0)
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
        main_status.text("üìä –°–±–æ—Ä–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
        step_status.text("–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...")
        
        all_data = []
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ —Å—Ç–∞—Ç—å–∏
        for doi, data in source_articles_data.items():
            data['type'] = 'source'
            data['source_doi'] = doi
            all_data.append(data)
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ü–∏—Ç–∏—Ä—É—é—â–∏–µ —Å—Ç–∞—Ç—å–∏
        total_citing = sum(len(citing_dois) for citing_dois in all_citing_articles.values())
        processed_citing = 0
        
        for source_doi, citing_dois in all_citing_articles.items():
            for citing_doi in citing_dois:
                if self.validate_doi(citing_doi):
                    citing_data = citing_articles_data.get(citing_doi, {})
                    if citing_data:
                        citing_data['type'] = 'citation'
                        citing_data['source_doi'] = source_doi
                        all_data.append(citing_data)
                
                processed_citing += 1
                if processed_citing % 10 == 0:
                    progress_pct = processed_citing / total_citing * 100
                    step_progress.progress(progress_pct / 100)
                    step_status.text(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π: {processed_citing}/{total_citing} ({progress_pct:.1f}%)")
        
        main_status.text("‚úÖ –ê–Ω–∞–ª–∏–∑ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π –∑–∞–≤–µ—Ä—à–µ–Ω!")
        step_status.text("")
        step_progress.empty()
        
        # –°–æ–∑–¥–∞–µ–º DataFrame –∏ –¥–æ–±–∞–≤–ª—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏
        result_df = pd.DataFrame(all_data)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏, –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
        expected_columns = [
            'doi', 'title', 'year', 'publication_year', 'authors', 'authors_with_initials', 
            'author_count', 'journal_full_name', 'journal_abbreviation', 'publisher',
            'citation_count_crossref', 'citation_count_openalex', 'annual_citation_rate_crossref',
            'annual_citation_rate_openalex', 'years_since_publication', 'affiliations', 
            'countries', 'altmetric_score', 'number_of_mentions', 'x_mentions', 'rss_blogs',
            'unique_accounts', 'type', 'source_doi'
        ]
        
        for col in expected_columns:
            if col not in result_df.columns:
                result_df[col] = None
        
        return result_df

    # –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ï –ú–ï–¢–û–î–´ –≠–ö–°–ü–û–†–¢–ê –í EXCEL –° –°–û–•–†–ê–ù–ï–ù–ò–ï–ú –í–û –í–†–ï–ú–ï–ù–ù–´–ï –§–ê–ô–õ–´
    def save_references_analysis_to_excel(self, references_df: pd.DataFrame, source_articles_df: pd.DataFrame,
                                        doi_list: List[str], total_references: int, unique_dois: int,
                                        all_titles: List[str]) -> str:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å—Å—ã–ª–æ–∫ –≤ Excel –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É"""
        try:
            # –°–æ–∑–¥–∞–µ–º workbook –≤ –ø–∞–º—è—Ç–∏
            wb = Workbook()
            wb.remove(wb.active)  # –£–¥–∞–ª—è–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π –ª–∏—Å—Ç

            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            unique_df = self.get_unique_references(references_df)
            duplicate_df = self.find_duplicate_references(references_df)
            
            # –ê–Ω–∞–ª–∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
            content_freq, compound_freq, scientific_freq = self.analyze_titles(all_titles)

            # –°–æ–∑–¥–∞–µ–º –≤–∫–ª–∞–¥–∫—É Report_Summary
            ws_summary = wb.create_sheet('Report_Summary')
            
            summary_content = f"""@MedvDmitry production

REFERENCES ANALYSIS REPORT

Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

ANALYSIS OVERVIEW
=================
Total source articles: {len(doi_list)}
Total references collected: {total_references}
Unique DOIs identified: {unique_dois}
Total references processed: {len(references_df) if not references_df.empty else 0}
Unique references: {len(unique_df) if not unique_df.empty else 0}
Successful references: {len(references_df[references_df['error'].isna()]) if not references_df.empty else 0}
Failed references: {len(references_df[references_df['error'].notna()]) if not references_df.empty else 0}
Unique authors: {len(self.analyze_authors_frequency(references_df)) if not references_df.empty else 0}
Unique journals: {len(self.analyze_journals_frequency(references_df)) if not references_df.empty else 0}
Unique affiliations: {len(self.analyze_affiliations_frequency(references_df)) if not references_df.empty else 0}
Unique countries: {len(self.analyze_countries_frequency(references_df)) if not references_df.empty else 0}
Duplicate references: {len(duplicate_df) if not duplicate_df.empty else 0}

DATA COMPLETENESS
=================
References with country data: {(len(references_df[references_df['countries'].isin(['Unknown', 'Error']) == False]) / len(references_df) * 100) if not references_df.empty else 0:.1f}%
References with affiliation data: {(len(references_df[references_df['affiliations'].isin(['Unknown', 'Error']) == False]) / len(references_df) * 100) if not references_df.empty else 0:.1f}%
References with altmetric data: {(len(references_df[references_df['altmetric_score'] > 0]) / len(references_df) * 100) if not references_df.empty else 0:.1f}%

AFFILIATION PROCESSING
======================
Affiliations normalized and grouped by organization
Similar affiliations merged together
Frequency counts reflect grouped organizations

ALTMETRIC METRICS INCLUDED
==========================
Altmetric Score: Overall attention score
Number of Mentions: Posts mentioning the article
X Mentions: Twitter/X accounts mentioning
RSS/Blogs: Blog and RSS feed mentions
Unique Accounts: Unique accounts across platforms

TITLE ANALYSIS
==============
Content words analyzed: {len(content_freq)}
Compound words identified: {len(compound_freq)}
Scientific stopwords found: {len(scientific_freq)}

DATA QUALITY NOTES
==================
Analysis focuses on references cited by the source articles
Combined data from Crossref and OpenAlex improves completeness
All standard statistical analyses performed (authors, journals, countries, etc.)
Error handling ensures report generation even with partial data
Affiliations normalized and grouped for consistent organization names
Altmetric metrics provide social media and online attention analysis
Title word analysis helps identify key research topics and trends
"""

            for line in summary_content.split('\n'):
                ws_summary.append([line])

            # –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã
            sheets_data = [
                ('Source_Articles', source_articles_df),
                ('All_References', references_df),
                ('All_Unique_References', unique_df),
                ('Duplicate_References', duplicate_df)
            ]

            # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ç–∞–±–ª–∏—Ü—ã
            analysis_methods = [
                ('Author_Frequency', self.analyze_authors_frequency),
                ('Journal_Frequency', self.analyze_journals_frequency),
                ('Affiliation_Frequency', self.analyze_affiliations_frequency),
                ('Country_Frequency', self.analyze_countries_frequency),
                ('Year_Distribution', self.analyze_year_distribution),
                ('5_Years_Period', self.analyze_five_year_periods)
            ]

            for sheet_name, method in analysis_methods:
                try:
                    result_df = method(references_df)
                    sheets_data.append((sheet_name, result_df))
                except Exception as e:
                    sheets_data.append((sheet_name, pd.DataFrame()))

            # –ê–Ω–∞–ª–∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
            try:
                title_word_data = []
                for i, (word, count) in enumerate(content_freq.most_common(50), 1):
                    title_word_data.append({'Category': 'Content_Words', 'Rank': i, 'Word': word, 'Frequency': count})
                for i, (word, count) in enumerate(compound_freq.most_common(50), 1):
                    title_word_data.append({'Category': 'Compound_Words', 'Rank': i, 'Word': word, 'Frequency': count})
                for i, (word, count) in enumerate(scientific_freq.most_common(50), 1):
                    title_word_data.append({'Category': 'Scientific_Stopwords', 'Rank': i, 'Word': word, 'Frequency': count})

                title_word_df = pd.DataFrame(title_word_data)
                sheets_data.append(('Title_Word_Frequency', title_word_df))
            except Exception as e:
                sheets_data.append(('Title_Word_Frequency', pd.DataFrame()))

            # –°–æ–∑–¥–∞–µ–º –≤—Å–µ –≤–∫–ª–∞–¥–∫–∏
            for sheet_name, df in sheets_data:
                try:
                    if not df.empty:
                        ws = wb.create_sheet(sheet_name)
                        for r in dataframe_to_rows(df, index=False, header=True):
                            ws.append(r)
                    else:
                        ws = wb.create_sheet(sheet_name)
                        ws.append([f"No data available for {sheet_name}"])
                except Exception as e:
                    pass

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"references_analysis_{timestamp}.xlsx"
            file_path = get_temp_file_path(filename)
            
            wb.save(file_path)
            
            return file_path

        except Exception as e:
            self.logger.error(f"Error saving Excel: {e}")
            # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª —Å –æ—à–∏–±–∫–æ–π
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"error_references_analysis_{timestamp}.xlsx"
            file_path = get_temp_file_path(filename)
            
            wb = Workbook()
            ws = wb.active
            ws.title = "Error"
            ws.append(["Error creating Excel file"])
            ws.append([str(e)])
            wb.save(file_path)
            
            return file_path

    def save_citations_analysis_to_excel(self, citations_df: pd.DataFrame, citing_details_df: pd.DataFrame,
                                       doi_list: List[str], citing_results: Dict, all_citing_titles: List[str]) -> str:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π –≤ Excel –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É"""
        try:
            wb = Workbook()
            wb.remove(wb.active)

            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            unique_citations_df = self.get_unique_citations(citations_df)
            duplicate_citations_df = self.find_duplicate_citations(citations_df)
            
            # –ê–Ω–∞–ª–∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
            content_freq, compound_freq, scientific_freq = self.analyze_titles(all_citing_titles)

            # –°–æ–∑–¥–∞–µ–º –≤–∫–ª–∞–¥–∫—É Report_Summary
            ws_summary = wb.create_sheet('Report_Summary_Citations')
            
            citing_info = ""
            if citing_results:
                citing_info = f"\nCitations per source article:"
                for doi, data in citing_results.items():
                    citing_info += f"\n  - {doi}: {data['count']} citations"

            summary_content = f"""@MedvDmitry production

CITATION ANALYSIS REPORT (CITING ARTICLES)

Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

ANALYSIS OVERVIEW
=================
Total source articles: {len(doi_list)}
Total citation relationships: {len(citations_df) if not citations_df.empty else 0}
Total unique citing articles: {len(unique_citations_df) if not unique_citations_df.empty else 0}
Successful citations: {len(citations_df[citations_df['error'].isna()]) if not citations_df.empty else 0}
Failed citations: {len(citations_df[citations_df['error'].notna()]) if not citations_df.empty else 0}
Unique authors: {len(self.analyze_authors_frequency(citations_df)) if not citations_df.empty else 0}
Unique journals: {len(self.analyze_journals_frequency(citations_df)) if not citations_df.empty else 0}
Unique affiliations: {len(self.analyze_affiliations_frequency(citations_df)) if not citations_df.empty else 0}
Unique countries: {len(self.analyze_countries_frequency(citations_df)) if not citations_df.empty else 0}
Duplicate citations: {len(duplicate_citations_df) if not duplicate_citations_df.empty else 0}

DATA COMPLETENESS
=================
Articles with country data: {(len(citations_df[citations_df['countries'].isin(['Unknown', 'Error']) == False]) / len(citations_df) * 100) if not citations_df.empty else 0:.1f}%
Articles with affiliation data: {(len(citations_df[citations_df['affiliations'].isin(['Unknown', 'Error']) == False]) / len(citations_df) * 100) if not citations_df.empty else 0:.1f}%

AFFILIATION PROCESSING
======================
Affiliations normalized and grouped by organization
Similar affiliations merged together
Frequency counts reflect grouped organizations

TITLE ANALYSIS
==============
Content words analyzed: {len(content_freq)}
Compound words identified: {len(compound_freq)}
Scientific stopwords found: {len(scientific_freq)}
{citing_info}

DATA QUALITY NOTES
==================
Analysis focuses on articles that cite the source articles
Combined data from Crossref and OpenAlex improves completeness
All standard statistical analyses performed (authors, journals, countries, etc.)
Error handling ensures report generation even with partial data
Duplicate citations show articles that cite multiple source articles
Affiliations normalized and grouped for consistent organization names
Altmetric metrics included for social media and online attention analysis
Title word analysis helps identify key research topics and trends in citing literature
"""

            for line in summary_content.split('\n'):
                ws_summary.append([line])

            # –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã
            sheets_data = [
                ('Source_Articles_Citations', citing_details_df),
                ('All_Citations', citations_df),
                ('All_Unique_Citations', unique_citations_df),
                ('Duplicate_Citations', duplicate_citations_df)
            ]

            # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ç–∞–±–ª–∏—Ü—ã
            analysis_methods = [
                ('Author_Frequency_Citations', self.analyze_authors_frequency),
                ('Journal_Frequency_Citations', self.analyze_journals_frequency),
                ('Affiliation_Frequency_Citations', self.analyze_affiliations_frequency),
                ('Country_Frequency_Citations', self.analyze_countries_frequency),
                ('Year_Distribution_Citations', self.analyze_year_distribution),
                ('5_Years_Period_Citations', self.analyze_five_year_periods)
            ]

            for sheet_name, method in analysis_methods:
                try:
                    result_df = method(citations_df)
                    sheets_data.append((sheet_name, result_df))
                except Exception as e:
                    sheets_data.append((sheet_name, pd.DataFrame()))

            # –ê–Ω–∞–ª–∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
            try:
                title_word_data = []
                for i, (word, count) in enumerate(content_freq.most_common(50), 1):
                    title_word_data.append({'Category': 'Content_Words', 'Rank': i, 'Word': word, 'Frequency': count})
                for i, (word, count) in enumerate(compound_freq.most_common(50), 1):
                    title_word_data.append({'Category': 'Compound_Words', 'Rank': i, 'Word': word, 'Frequency': count})
                for i, (word, count) in enumerate(scientific_freq.most_common(50), 1):
                    title_word_data.append({'Category': 'Scientific_Stopwords', 'Rank': i, 'Word': word, 'Frequency': count})

                title_word_df = pd.DataFrame(title_word_data)
                sheets_data.append(('Title_Word_Frequency_Citations', title_word_df))
            except Exception as e:
                sheets_data.append(('Title_Word_Frequency_Citations', pd.DataFrame()))

            # –°–æ–∑–¥–∞–µ–º –≤—Å–µ –≤–∫–ª–∞–¥–∫–∏
            for sheet_name, df in sheets_data:
                try:
                    if not df.empty:
                        ws = wb.create_sheet(sheet_name)
                        for r in dataframe_to_rows(df, index=False, header=True):
                            ws.append(r)
                    else:
                        ws = wb.create_sheet(sheet_name)
                        ws.append([f"No data available for {sheet_name}"])
                except Exception as e:
                    pass

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"citations_analysis_{timestamp}.xlsx"
            file_path = get_temp_file_path(filename)
            
            wb.save(file_path)
            
            return file_path

        except Exception as e:
            self.logger.error(f"Error saving citations Excel: {e}")
            # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª —Å –æ—à–∏–±–∫–æ–π
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"error_citations_analysis_{timestamp}.xlsx"
            file_path = get_temp_file_path(filename)
            
            wb = Workbook()
            ws = wb.active
            ws.title = "Error"
            ws.append(["Error creating Excel file"])
            ws.append([str(e)])
            wb.save(file_path)
            
            return file_path

def main():
    st.title("üìö Refs/Cits Analysis - Full Professional Version")
    st.markdown("üî¨ –ü–æ–ª–Ω–æ—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å—Å—ã–ª–æ–∫ –∏ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π")
    
    # –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
    cleanup_temp_files()
    
    if not NLTK_AVAILABLE:
        st.warning("‚ö†Ô∏è NLTK –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞.")
    
    analyzer = FullCitationAnalyzer()
    
    # –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
    with st.sidebar:
        st.header("‚ÑπÔ∏è –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è")
        st.info("""
        **–ü–æ–ª–Ω—ã–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª:**
        - –ê–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö —Å—Å—ã–ª–æ–∫ –∏ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π
        - –î–∞–Ω–Ω—ã–µ –∏–∑ Crossref, OpenAlex, Altmetric
        - –ê–Ω–∞–ª–∏–∑ –∞—Ñ—Ñ–∏–ª–∏–∞—Ü–∏–π –∏ —Å—Ç—Ä–∞–Ω
        - –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∞–≤—Ç–æ—Ä–∞–º –∏ –∂—É—Ä–Ω–∞–ª–∞–º
        - –ê–ª—å—Ç–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏
        - –ê–Ω–∞–ª–∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ —á–µ—Ä–µ–∑ NLTK
        - –ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–µ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ
        - –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫
        - –≠–∫—Å–ø–æ—Ä—Ç –≤ Excel —Å –º–Ω–æ–∂–µ—Å—Ç–≤–æ–º –≤–∫–ª–∞–¥–æ–∫
        - –ë–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö
        """)
        
        st.markdown("---")
        st.markdown("### üìä –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã DOI")
        st.markdown("""
        - `10.1234/abcd.1234`
        - `https://doi.org/10.1234/abcd.1234`
        - `doi:10.1234/abcd.1234`
        - –ò –º–Ω–æ–≥–∏–µ –¥—Ä—É–≥–∏–µ —Ñ–æ—Ä–º–∞—Ç—ã
        """)
        
        if st.button("üîÑ –û—á–∏—Å—Ç–∏—Ç—å –∫–µ—à"):
            analyzer.invalidate_cache()
            st.success("‚úÖ –ö–µ—à –æ—á–∏—â–µ–Ω!")
    
    # –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
    tab1, tab2, tab3 = st.tabs(["üì• –í–≤–æ–¥ –¥–∞–Ω–Ω—ã—Ö", "üîó –ê–Ω–∞–ª–∏–∑ —Å—Å—ã–ª–æ–∫", "üìà –ê–Ω–∞–ª–∏–∑ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π"])
    
    with tab1:
        st.header("–í–≤–æ–¥ DOI —Å—Ç–∞—Ç–µ–π")
        
        input_method = st.radio("–°–ø–æ—Å–æ–± –≤–≤–æ–¥–∞:", ["üìù –¢–µ–∫—Å—Ç", "üìÅ –§–∞–π–ª"])
        
        if input_method == "üìù –¢–µ–∫—Å—Ç":
            doi_input = st.text_area(
                "–í–≤–µ–¥–∏—Ç–µ DOI —Å—Ç–∞—Ç–µ–π:",
                height=200,
                placeholder="–í–≤–µ–¥–∏—Ç–µ DOI —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é, —Ç–æ—á–∫—É —Å –∑–∞–ø—è—Ç–æ–π –∏–ª–∏ —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏:\n10.1038/s41586-023-06924-6\n10.1126/science.abl8921\n10.1016/j.cell.2023.08.012\n10.1038/s41557-023-01282-2\nhttps://doi.org/10.1038/s41586-023-06924-6\ndoi:10.1126/science.abl8921"
            )
            
            if st.button("üîç –ü—Ä–æ–≤–µ—Ä–∏—Ç—å DOI", key="validate"):
                if doi_input:
                    dois = analyzer.parse_doi_input(doi_input)
                    if dois:
                        st.success(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(dois)} –≤–∞–ª–∏–¥–Ω—ã—Ö DOI:")
                        
                        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º DOI –≤ –≤–∏–¥–µ —Ç–∞–±–ª–∏—Ü—ã
                        doi_df = pd.DataFrame(dois, columns=['DOI'])
                        st.dataframe(doi_df, use_container_width=True)
                        
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ session state
                        st.session_state.dois = dois
                        st.session_state.dois_input = doi_input
                    else:
                        st.error("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –≤–∞–ª–∏–¥–Ω—ã—Ö DOI. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–æ—Ä–º–∞—Ç.")
        
        else:
            uploaded_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª —Å DOI", type=['txt', 'csv'])
            if uploaded_file:
                content = uploaded_file.getvalue().decode()
                dois = analyzer.parse_doi_input(content)
                if dois:
                    st.success(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(dois)} –≤–∞–ª–∏–¥–Ω—ã—Ö DOI –≤ —Ñ–∞–π–ª–µ")
                    
                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º DOI –≤ –≤–∏–¥–µ —Ç–∞–±–ª–∏—Ü—ã
                    doi_df = pd.DataFrame(dois, columns=['DOI'])
                    st.dataframe(doi_df, use_container_width=True)
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ session state
                    st.session_state.dois = dois
                    st.session_state.dois_input = content
                else:
                    st.error("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –≤–∞–ª–∏–¥–Ω—ã—Ö DOI –≤ —Ñ–∞–π–ª–µ.")
    
    with tab2:
        st.header("üîó –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å—Å—ã–ª–æ–∫ (References)")
        
        if 'dois' in st.session_state and st.session_state.dois:
            dois = st.session_state.dois
            
            st.info(f"üîç –ë—É–¥–µ—Ç –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(dois)} —Å—Ç–∞—Ç–µ–π –∏ –í–°–ï –∏—Ö —Å—Å—ã–ª–∫–∏")
            
            if st.button("üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å—Å—ã–ª–æ–∫", type="primary", key="refs_full"):
                with st.spinner("‚ö° –í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å—Å—ã–ª–æ–∫... –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –≤—Ä–µ–º—è"):
                    references_df = analyzer.analyze_references_comprehensive(dois)
                
                if not references_df.empty:
                    st.success("‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ session state
                    st.session_state.references_df = references_df
                    st.session_state.references_analysis_complete = True
                    
                    # –û—Å–Ω–æ–≤–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                    st.subheader("üìä –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")
                    col1, col2, col3, col4 = st.columns(4)
                    
                    with col1:
                        total_articles = len(references_df)
                        st.metric("–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π", total_articles)
                    with col2:
                        source_articles = len(references_df[references_df['type'] == 'source'])
                        st.metric("–ò—Å—Ö–æ–¥–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π", source_articles)
                    with col3:
                        references_count = len(references_df[references_df['type'] == 'reference'])
                        st.metric("–°—Å—ã–ª–æ–∫", references_count)
                    with col4:
                        unique_dois = references_df['doi'].nunique()
                        st.metric("–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö DOI", unique_dois)
                    
                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        unique_journals = references_df['journal_abbreviation'].nunique()
                        st.metric("–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∂—É—Ä–Ω–∞–ª–æ–≤", unique_journals)
                    with col2:
                        unique_authors = len(analyzer.analyze_authors_frequency(references_df))
                        st.metric("–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∞–≤—Ç–æ—Ä–æ–≤", unique_authors)
                    with col3:
                        unique_countries = len(analyzer.analyze_countries_frequency(references_df))
                        st.metric("–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç—Ä–∞–Ω", unique_countries)
                    with col4:
                        avg_citations = references_df['citation_count_openalex'].mean()
                        st.metric("–°—Ä–µ–¥–Ω–∏–µ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è", f"{avg_citations:.1f}")
                    
                    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
                    st.subheader("üìà –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏")
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –≥–æ–¥–∞–º
                        year_df = analyzer.analyze_year_distribution(references_df)
                        if not year_df.empty and 'frequency_total' in year_df.columns:
                            viz_df = year_df[['year', 'frequency_total']].set_index('year')
                            st.bar_chart(viz_df.head(15))
                        else:
                            st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –ø–æ –≥–æ–¥–∞–º")
                    
                    with col2:
                        # –¢–æ–ø –∂—É—Ä–Ω–∞–ª–æ–≤
                        journal_df = analyzer.analyze_journals_frequency(references_df)
                        if not journal_df.empty and 'frequency_total' in journal_df.columns:
                            viz_df = journal_df[['journal_abbreviation', 'frequency_total']].set_index('journal_abbreviation')
                            st.bar_chart(viz_df.head(10))
                        else:
                            st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –ø–æ –∂—É—Ä–Ω–∞–ª–∞–º")
                    
                    # –î–µ—Ç–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                    st.subheader("üìã –î–µ—Ç–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
                    
                    # –§–∏–ª—å—Ç—Ä—ã
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        record_type = st.selectbox("–¢–∏–ø –∑–∞–ø–∏—Å–µ–π:", ["–í—Å–µ", "–¢–æ–ª—å–∫–æ –∏—Å—Ö–æ–¥–Ω—ã–µ", "–¢–æ–ª—å–∫–æ —Å—Å—ã–ª–∫–∏"])
                    with col2:
                        sort_by = st.selectbox("–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞:", ["doi", "year", "citation_count_openalex", "altmetric_score"])
                    with col3:
                        sort_order = st.selectbox("–ü–æ—Ä—è–¥–æ–∫:", ["–ü–æ —É–±—ã–≤–∞–Ω–∏—é", "–ü–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏—é"])
                    
                    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã
                    filtered_df = references_df
                    if record_type == "–¢–æ–ª—å–∫–æ –∏—Å—Ö–æ–¥–Ω—ã–µ":
                        filtered_df = references_df[references_df['type'] == 'source']
                    elif record_type == "–¢–æ–ª—å–∫–æ —Å—Å—ã–ª–∫–∏":
                        filtered_df = references_df[references_df['type'] == 'reference']
                    
                    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞
                    ascending = sort_order == "–ü–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏—é"
                    filtered_df = filtered_df.sort_values(sort_by, ascending=ascending)
                    
                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
                    st.dataframe(filtered_df, use_container_width=True, height=400)
                    
                    # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ç–∞–±–ª–∏—Ü—ã
                    st.subheader("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ç–∞–±–ª–∏—Ü—ã")
                    
                    tab1, tab2, tab3, tab4, tab5, tab6, tab7 = st.tabs([
                        "–ê–≤—Ç–æ—Ä—ã", "–ñ—É—Ä–Ω–∞–ª—ã", "–ê—Ñ—Ñ–∏–ª–∏–∞—Ü–∏–∏", "–°—Ç—Ä–∞–Ω—ã", "–ì–æ–¥—ã", "5-–ª–µ—Ç–Ω–∏–µ –ø–µ—Ä–∏–æ–¥—ã", "–î—É–±–ª–∏–∫–∞—Ç—ã"
                    ])
                    
                    with tab1:
                        authors_df = analyzer.analyze_authors_frequency(references_df)
                        if not authors_df.empty:
                            st.dataframe(authors_df.head(20), use_container_width=True)
                    
                    with tab2:
                        journals_df = analyzer.analyze_journals_frequency(references_df)
                        if not journals_df.empty:
                            st.dataframe(journals_df.head(20), use_container_width=True)
                    
                    with tab3:
                        affiliations_df = analyzer.analyze_affiliations_frequency(references_df)
                        if not affiliations_df.empty:
                            st.dataframe(affiliations_df.head(20), use_container_width=True)
                    
                    with tab4:
                        countries_df = analyzer.analyze_countries_frequency(references_df)
                        if not countries_df.empty:
                            st.dataframe(countries_df.head(20), use_container_width=True)
                    
                    with tab5:
                        years_df = analyzer.analyze_year_distribution(references_df)
                        if not years_df.empty:
                            st.dataframe(years_df.head(20), use_container_width=True)
                    
                    with tab6:
                        periods_df = analyzer.analyze_five_year_periods(references_df)
                        if not periods_df.empty:
                            st.dataframe(periods_df.head(20), use_container_width=True)
                    
                    with tab7:
                        duplicates_df = analyzer.find_duplicate_references(references_df)
                        if not duplicates_df.empty:
                            st.dataframe(duplicates_df.head(20), use_container_width=True)
                    
                    # –≠–∫—Å–ø–æ—Ä—Ç –≤ Excel
                    st.subheader("üíæ –≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö")
                    
                    if st.button("üìä –°–æ–∑–¥–∞—Ç—å –ø–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç Excel", key="excel_refs"):
                        with st.spinner("–°–æ–∑–¥–∞–Ω–∏–µ Excel –æ—Ç—á–µ—Ç–∞..."):
                            source_articles_df = references_df[references_df['type'] == 'source']
                            all_titles = references_df['title'].tolist()
                            excel_file_path = analyzer.save_references_analysis_to_excel(
                                references_df, source_articles_df, dois, 
                                len(references_df[references_df['type'] == 'reference']),
                                references_df['doi'].nunique(), all_titles
                            )
                            
                            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –≤ session state
                            st.session_state.excel_refs_path = excel_file_path
                            st.session_state.excel_refs_filename = os.path.basename(excel_file_path)
                            
                            st.success("‚úÖ Excel –æ—Ç—á–µ—Ç —Å–æ–∑–¥–∞–Ω!")
                    
                    # –ö–Ω–æ–ø–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è - –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è —Ñ–∞–π–ª–∞
                    if 'excel_refs_path' in st.session_state:
                        # –°–æ–∑–¥–∞–µ–º —Å—Å—ã–ª–∫—É –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
                        download_link = create_download_link(
                            st.session_state.excel_refs_path,
                            st.session_state.excel_refs_filename,
                            "üì• –°–∫–∞—á–∞—Ç—å –ø–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç (Excel)"
                        )
                        st.markdown(download_link, unsafe_allow_html=True)
                        
                        # –¢–∞–∫–∂–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –∫–Ω–æ–ø–∫—É –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏
                        with open(st.session_state.excel_refs_path, 'rb') as f:
                            st.download_button(
                                "üì• –°–∫–∞—á–∞—Ç—å –æ—Ç—á–µ—Ç (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–ø–æ—Å–æ–±)",
                                f,
                                st.session_state.excel_refs_filename,
                                "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                                key='download_excel_refs'
                            )
                else:
                    st.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ")
        else:
            st.warning("‚ö†Ô∏è –°–Ω–∞—á–∞–ª–∞ –≤–≤–µ–¥–∏—Ç–µ DOI –Ω–∞ –≤–∫–ª–∞–¥–∫–µ '–í–≤–æ–¥ –¥–∞–Ω–Ω—ã—Ö'")
    
    with tab3:
        st.header("üìà –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π (Citations)")
        
        if 'dois' in st.session_state and st.session_state.dois:
            dois = st.session_state.dois
            
            st.info(f"üîç –ë—É–¥–µ—Ç –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(dois)} —Å—Ç–∞—Ç–µ–π –∏ –í–°–ï –∏—Ö —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
            
            if st.button("üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π", type="primary", key="cits_full"):
                with st.spinner("‚ö° –í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π... –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –≤—Ä–µ–º—è"):
                    # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                    citations_df = analyzer.analyze_citations_comprehensive(dois)
                    citing_details_df = citations_df[citations_df['type'] == 'citation'].copy()
                    citing_results = {}
                    for doi in dois:
                        citing_count = len(citations_df[
                            (citations_df['type'] == 'citation') & 
                            (citations_df['source_doi'] == doi)
                        ])
                        citing_results[doi] = {'count': citing_count, 'citing_dois': []}
                
                if not citations_df.empty:
                    st.success("‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ session state
                    st.session_state.citations_df = citations_df
                    st.session_state.citations_analysis_complete = True
                    
                    # –û—Å–Ω–æ–≤–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                    st.subheader("üìä –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")
                    col1, col2, col3, col4 = st.columns(4)
                    
                    with col1:
                        total_articles = len(citations_df)
                        st.metric("–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π", total_articles)
                    with col2:
                        source_articles = len(citations_df[citations_df['type'] == 'source'])
                        st.metric("–ò—Å—Ö–æ–¥–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π", source_articles)
                    with col3:
                        citations_count = len(citations_df[citations_df['type'] == 'citation'])
                        st.metric("–¶–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π", citations_count)
                    with col4:
                        unique_dois = citations_df['doi'].nunique()
                        st.metric("–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö DOI", unique_dois)
                    
                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        citing_journals = citations_df[citations_df['type'] == 'citation']['journal_abbreviation'].nunique()
                        st.metric("–¶–∏—Ç–∏—Ä—É—é—â–∏—Ö –∂—É—Ä–Ω–∞–ª–æ–≤", citing_journals)
                    with col2:
                        citing_authors = len(analyzer.analyze_authors_frequency(citations_df[citations_df['type'] == 'citation']))
                        st.metric("–¶–∏—Ç–∏—Ä—É—é—â–∏—Ö –∞–≤—Ç–æ—Ä–æ–≤", citing_authors)
                    with col3:
                        citing_countries = len(analyzer.analyze_countries_frequency(citations_df[citations_df['type'] == 'citation']))
                        st.metric("–¶–∏—Ç–∏—Ä—É—é—â–∏—Ö —Å—Ç—Ä–∞–Ω", citing_countries)
                    with col4:
                        recent_year = str(datetime.now().year)
                        recent_citations = len(citations_df[
                            (citations_df['type'] == 'citation') & 
                            (citations_df['year'] == recent_year)
                        ])
                        st.metric(f"–¶–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π {recent_year}", recent_citations)
                    
                    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
                    st.subheader("üìà –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏")
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –≥–æ–¥–∞–º
                        year_df = analyzer.analyze_year_distribution(citations_df)
                        if not year_df.empty and 'frequency_total' in year_df.columns:
                            viz_df = year_df[['year', 'frequency_total']].set_index('year')
                            st.bar_chart(viz_df.head(15))
                        else:
                            st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –ø–æ –≥–æ–¥–∞–º")
                    
                    with col2:
                        # –¢–æ–ø —Ü–∏—Ç–∏—Ä—É—é—â–∏—Ö –∂—É—Ä–Ω–∞–ª–æ–≤
                        citing_journals_df = analyzer.analyze_journals_frequency(citations_df[citations_df['type'] == 'citation'])
                        if not citing_journals_df.empty and 'frequency_total' in citing_journals_df.columns:
                            viz_df = citing_journals_df[['journal_abbreviation', 'frequency_total']].set_index('journal_abbreviation')
                            st.bar_chart(viz_df.head(10))
                        else:
                            st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –ø–æ –∂—É—Ä–Ω–∞–ª–∞–º")
                    
                    # –î–µ—Ç–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                    st.subheader("üìã –î–µ—Ç–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
                    
                    # –§–∏–ª—å—Ç—Ä—ã
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        record_type = st.selectbox("–¢–∏–ø –∑–∞–ø–∏—Å–µ–π:", ["–í—Å–µ", "–¢–æ–ª—å–∫–æ –∏—Å—Ö–æ–¥–Ω—ã–µ", "–¢–æ–ª—å–∫–æ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"], key='cit_type')
                    with col2:
                        sort_by = st.selectbox("–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞:", ["doi", "year", "citation_count_openalex", "altmetric_score"], key='cit_sort')
                    with col3:
                        sort_order = st.selectbox("–ü–æ—Ä—è–¥–æ–∫:", ["–ü–æ —É–±—ã–≤–∞–Ω–∏—é", "–ü–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏—é"], key='cit_order')
                    
                    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã
                    filtered_df = citations_df
                    if record_type == "–¢–æ–ª—å–∫–æ –∏—Å—Ö–æ–¥–Ω—ã–µ":
                        filtered_df = citations_df[citations_df['type'] == 'source']
                    elif record_type == "–¢–æ–ª—å–∫–æ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è":
                        filtered_df = citations_df[citations_df['type'] == 'citation']
                    
                    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞
                    ascending = sort_order == "–ü–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏—é"
                    filtered_df = filtered_df.sort_values(sort_by, ascending=ascending)
                    
                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
                    st.dataframe(filtered_df, use_container_width=True, height=400)
                    
                    # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ç–∞–±–ª–∏—Ü—ã
                    st.subheader("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ç–∞–±–ª–∏—Ü—ã")
                    
                    tab1, tab2, tab3, tab4, tab5, tab6, tab7 = st.tabs([
                        "–¶–∏—Ç–∏—Ä—É—é—â–∏–µ –∞–≤—Ç–æ—Ä—ã", "–¶–∏—Ç–∏—Ä—É—é—â–∏–µ –∂—É—Ä–Ω–∞–ª—ã", "–¶–∏—Ç–∏—Ä—É—é—â–∏–µ –∞—Ñ—Ñ–∏–ª–∏–∞—Ü–∏–∏", 
                        "–¶–∏—Ç–∏—Ä—É—é—â–∏–µ —Å—Ç—Ä–∞–Ω—ã", "–ì–æ–¥—ã —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π", "5-–ª–µ—Ç–Ω–∏–µ –ø–µ—Ä–∏–æ–¥—ã", "–î—É–±–ª–∏–∫–∞—Ç—ã"
                    ])
                    
                    with tab1:
                        citing_authors_df = analyzer.analyze_authors_frequency(citations_df[citations_df['type'] == 'citation'])
                        if not citing_authors_df.empty:
                            st.dataframe(citing_authors_df.head(20), use_container_width=True)
                    
                    with tab2:
                        citing_journals_df = analyzer.analyze_journals_frequency(citations_df[citations_df['type'] == 'citation'])
                        if not citing_journals_df.empty:
                            st.dataframe(citing_journals_df.head(20), use_container_width=True)
                    
                    with tab3:
                        citing_affiliations_df = analyzer.analyze_affiliations_frequency(citations_df[citations_df['type'] == 'citation'])
                        if not citing_affiliations_df.empty:
                            st.dataframe(citing_affiliations_df.head(20), use_container_width=True)
                    
                    with tab4:
                        citing_countries_df = analyzer.analyze_countries_frequency(citations_df[citations_df['type'] == 'citation'])
                        if not citing_countries_df.empty:
                            st.dataframe(citing_countries_df.head(20), use_container_width=True)
                    
                    with tab5:
                        citing_years_df = analyzer.analyze_year_distribution(citations_df[citations_df['type'] == 'citation'])
                        if not citing_years_df.empty:
                            st.dataframe(citing_years_df.head(20), use_container_width=True)
                    
                    with tab6:
                        citing_periods_df = analyzer.analyze_five_year_periods(citations_df[citations_df['type'] == 'citation'])
                        if not citing_periods_df.empty:
                            st.dataframe(citing_periods_df.head(20), use_container_width=True)
                    
                    with tab7:
                        citing_duplicates_df = analyzer.find_duplicate_citations(citations_df)
                        if not citing_duplicates_df.empty:
                            st.dataframe(citing_duplicates_df.head(20), use_container_width=True)
                    
                    # –≠–∫—Å–ø–æ—Ä—Ç –≤ Excel
                    st.subheader("üíæ –≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö")
                    
                    if st.button("üìä –°–æ–∑–¥–∞—Ç—å –ø–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç Excel", key="excel_cits"):
                        with st.spinner("–°–æ–∑–¥–∞–Ω–∏–µ Excel –æ—Ç—á–µ—Ç–∞..."):
                            all_citing_titles = citations_df[citations_df['type'] == 'citation']['title'].tolist()
                            excel_file_path = analyzer.save_citations_analysis_to_excel(
                                citations_df, citing_details_df, dois, citing_results, all_citing_titles
                            )
                            
                            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –≤ session state
                            st.session_state.excel_cits_path = excel_file_path
                            st.session_state.excel_cits_filename = os.path.basename(excel_file_path)
                            
                            st.success("‚úÖ Excel –æ—Ç—á–µ—Ç —Å–æ–∑–¥–∞–Ω!")
                    
                    # –ö–Ω–æ–ø–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è - –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è —Ñ–∞–π–ª–∞
                    if 'excel_cits_path' in st.session_state:
                        # –°–æ–∑–¥–∞–µ–º —Å—Å—ã–ª–∫—É –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
                        download_link = create_download_link(
                            st.session_state.excel_cits_path,
                            st.session_state.excel_cits_filename,
                            "üì• –°–∫–∞—á–∞—Ç—å –ø–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç (Excel)"
                        )
                        st.markdown(download_link, unsafe_allow_html=True)
                        
                        # –¢–∞–∫–∂–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –∫–Ω–æ–ø–∫—É –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏
                        with open(st.session_state.excel_cits_path, 'rb') as f:
                            st.download_button(
                                "üì• –°–∫–∞—á–∞—Ç—å –æ—Ç—á–µ—Ç (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–ø–æ—Å–æ–±)",
                                f,
                                st.session_state.excel_cits_filename,
                                "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                                key='download_excel_cits'
                            )
                else:
                    st.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ")
        else:
            st.warning("‚ö†Ô∏è –°–Ω–∞—á–∞–ª–∞ –≤–≤–µ–¥–∏—Ç–µ DOI –Ω–∞ –≤–∫–ª–∞–¥–∫–µ '–í–≤–æ–¥ –¥–∞–Ω–Ω—ã—Ö'")

if __name__ == "__main__":
    main()

